{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Junk Food Classification with KNN\n","\n","This notebook implements a **K-Nearest Neighbors (KNN)** classifier for image classification using a **COCO-style dataset**.  \n","The goal is to compare the performance of KNN against YOLO and CLIP pipelines using the same dataset and consistent evaluation metrics.\n","\n","Running the pipeline is relatively quick, since it uses a pre-trained model (RestNet-50)"],"metadata":{"id":"EvlC2Rnni7oi"}},{"cell_type":"markdown","source":["## Before you start\n","\n","Make sure you have access to GPU. In case of any problems, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, click `Save` and try again."],"metadata":{"id":"gcbOFYLzjV6B"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjlhPWyOjWsp","executionInfo":{"status":"ok","timestamp":1766958913888,"user_tz":360,"elapsed":240,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"5fc59fd1-e129-4b83-e874-31ca972ff071"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Dec 28 21:55:14 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["import os\n","HOME = os.getcwd()\n","print(\"HOME:\", HOME)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5H3H-YHDkdqz","executionInfo":{"status":"ok","timestamp":1766958913910,"user_tz":360,"elapsed":20,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"13d228ea-8581-44df-ad5e-1088fc704976"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["HOME: /content\n"]}]},{"cell_type":"code","source":["!mkdir -p {HOME}/datasets\n","%cd {HOME}/datasets\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xe-7ZMI6kebP","executionInfo":{"status":"ok","timestamp":1766958914049,"user_tz":360,"elapsed":129,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"ff15289b-3a32-42ee-f5a5-4486dc6962b1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/datasets\n"]}]},{"cell_type":"markdown","source":["## Install packages using pip"],"metadata":{"id":"mH1xxtULn2xV"}},{"cell_type":"code","source":["!pip install roboflow==1.2.11 torch==2.9.0 torchvision==0.24.0 scikit-learn==1.6.1 tqdm==4.67.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xvTEqLaOkjPL","executionInfo":{"status":"ok","timestamp":1766958937335,"user_tz":360,"elapsed":23250,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"ec90b92f-3d9c-472c-fc4c-0a49d80c67d3","collapsed":true},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: roboflow==1.2.11 in /usr/local/lib/python3.12/dist-packages (1.2.11)\n","Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: torchvision==0.24.0 in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2025.11.12)\n","Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (3.7)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.4.9)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (3.10.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.0.2)\n","Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (4.10.0.84)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (11.3.0)\n","Requirement already satisfied: pi-heif<2 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.1.1)\n","Requirement already satisfied: pillow-avif-plugin<2 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.5.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.9.0.post0)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.2.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.32.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.17.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.5.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (6.0.3)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.0.0)\n","Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.5.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.5.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (3.6.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0) (3.0.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (1.3.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (4.61.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow==1.2.11) (3.4.4)\n"]}]},{"cell_type":"markdown","source":["## Download dataset from Roboflow\n","\n","Don't forget to change the `API_KEY` with your dataset key.\n","\n","We replicate your original dataset setup. Even though the dataset is labeled for object detection, we’ll use the full image classification approach with KNN. Labels will be derived from the most frequent class per image."],"metadata":{"id":"_CCGi6EWnsMN"}},{"cell_type":"code","source":["from roboflow import Roboflow\n","from google.colab import userdata\n","\n","rf = Roboflow(api_key=userdata.get('ROBOFLOW_API_KEY'))\n","project = rf.workspace(userdata.get('ROBOFLOW_WORKSPACE_ID')).project(userdata.get('ROBOFLOW_PROJECT_ID'))\n","version = project.version(userdata.get('ROBOFLOW_DATASET_VERSION'))\n","dataset = version.download(\"coco\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XM4b5M09kj3D","executionInfo":{"status":"ok","timestamp":1766958940304,"user_tz":360,"elapsed":2966,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"add73abc-cd6b-4977-c524-fc9d3b9e778d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]}]},{"cell_type":"code","source":["%cd {HOME}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71fxf9Tsktei","executionInfo":{"status":"ok","timestamp":1766958940312,"user_tz":360,"elapsed":6,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"c1c45763-f9e6-4d62-d56a-704156820abb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"markdown","source":["## Dataset Loading and Label Extraction\n","\n","We will use a classification model. So, for labeling, we use two classes: junk-food-ad and non-junk-food-ad. Given the fact that the dataset is multiclass, the rule is: if there is at least one bounding box belonging to a particular image, it's junk-food-ad. Otherwise, it's non-junk-food-ad"],"metadata":{"id":"LM3porFAovJj"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"_QaNLLGlie3V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766958940428,"user_tz":360,"elapsed":115,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"054c12d2-27a0-4c79-d691-b244a81616e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","DATASET SUMMARY\n","==================================================\n","\n","Classes: ['junk-food-ad', 'non-junk-food-ad']\n","\n","Dataset parts processed:\n","\n","TRAIN:\n","  Total images: 4614\n","  Total labels: 4614\n","  Label distribution:\n","    - junk-food-ad: 2925 (63.4%)\n","    - non-junk-food-ad: 1689 (36.6%)\n","\n","VALID:\n","  Total images: 440\n","  Total labels: 440\n","  Label distribution:\n","    - junk-food-ad: 277 (63.0%)\n","    - non-junk-food-ad: 163 (37.0%)\n","\n","TEST:\n","  Total images: 218\n","  Total labels: 218\n","  Label distribution:\n","    - junk-food-ad: 131 (60.1%)\n","    - non-junk-food-ad: 87 (39.9%)\n"]}],"source":["import json\n","import os\n","from pathlib import Path\n","from typing import Dict, List, Tuple\n","\n","\n","def load_coco_annotations(json_path: str) -> Dict:\n","    with open(json_path, 'r') as f:\n","        return json.load(f)\n","\n","\n","def process_dataset_part(\n","    part_dir: str,\n","    positive_class: str,\n","    negative_class: str,\n","    annotations_filename: str = \"_annotations.coco.json\"\n",") -> Tuple[List[str], List[str]]:\n","    annotations_path = os.path.join(part_dir, annotations_filename)\n","\n","    if not os.path.exists(annotations_path):\n","        raise FileNotFoundError(f\"Annotations file not found: {annotations_path}\")\n","\n","    # Load annotations\n","    coco_data = load_coco_annotations(annotations_path)\n","\n","    # Create a mapping of image_id to whether it has annotations\n","    images_with_boxes = set()\n","    for annotation in coco_data['annotations']:\n","        images_with_boxes.add(annotation['image_id'])\n","\n","    # Process images in order\n","    image_paths = []\n","    labels = []\n","\n","    for image_info in coco_data['images']:\n","        image_id = image_info['id']\n","        file_name = image_info['file_name']\n","\n","        image_path = os.path.join(part_dir, file_name)\n","        image_paths.append(image_path)\n","\n","        # Assign binary label based on presence of bounding boxes\n","        if image_id in images_with_boxes:\n","            labels.append(positive_class)\n","        else:\n","            labels.append(negative_class)\n","\n","    return image_paths, labels\n","\n","\n","def process_full_dataset(\n","    dataset_root: str,\n","    positive_class: str = 'junk-food-ad',\n","    negative_class: str = 'non-junk-food-ad',\n","    parts: List[str] = ['train', 'valid', 'test']\n",") -> Tuple[Dict[str, List[str]], Dict[str, List[str]], List[str]]:\n","    # Define binary classes\n","    classes = [positive_class, negative_class]\n","\n","    all_image_paths = {}\n","    all_labels = {}\n","\n","    for part in parts:\n","        part_dir = os.path.join(dataset_root, part)\n","\n","        if not os.path.exists(part_dir):\n","            print(f\"Warning: Directory not found: {part_dir}. Skipping...\")\n","            continue\n","\n","        image_paths, labels = process_dataset_part(part_dir, positive_class, negative_class)\n","\n","        all_image_paths[part] = image_paths\n","        all_labels[part] = labels\n","\n","    return all_image_paths, all_labels, classes\n","\n","\n","image_paths_dict, labels_dict, classes = process_full_dataset(dataset.location)\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"DATASET SUMMARY\")\n","print(\"=\"*50)\n","print(f\"\\nClasses: {classes}\")\n","print(f\"\\nDataset parts processed:\")\n","\n","for part in image_paths_dict.keys():\n","    print(f\"\\n{part.upper()}:\")\n","    print(f\"  Total images: {len(image_paths_dict[part])}\")\n","    print(f\"  Total labels: {len(labels_dict[part])}\")\n","    print(f\"  Label distribution:\")\n","    for cls in classes:\n","        count = labels_dict[part].count(cls)\n","        percentage = (count / len(labels_dict[part]) * 100) if labels_dict[part] else 0\n","        print(f\"    - {cls}: {count} ({percentage:.1f}%)\")\n","\n","\n","# Access individual parts like this:\n","# train_images = image_paths_dict['train']\n","# train_labels = labels_dict['train']\n","# valid_images = image_paths_dict['valid']\n","# valid_labels = labels_dict['valid']\n","# test_images = image_paths_dict['test']\n","# test_labels = labels_dict['test']"]},{"cell_type":"markdown","source":["## Feature Extraction of train set using pretrained models\n","\n","KNN itself cannot extract visual features, it only compares numeric vectors.  \n","Therefore, we use **pre-trained** models (without their classification heads) to extract image embeddings of train set.\n","\n","These embeddings (feature vectors) represent each image in a high-dimensional space that captures visual similarity.  \n","The extracted features are stored as a NumPy matrix and later fed into the KNN classifier."],"metadata":{"id":"ZJqUcsBxo6X-"}},{"cell_type":"code","source":["import torch\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from PIL import Image\n","from tqdm import tqdm\n","import numpy as np\n","\n","def extract_features(image_paths, model, feature_dim, transform, model_name):\n","    features = []\n","    with torch.no_grad():\n","        for path in tqdm(image_paths, desc=f\"Extracting features - {model_name}\"):\n","            try:\n","                img = Image.open(path).convert(\"RGB\")\n","                tensor = transform(img).unsqueeze(0).to(device)\n","                feat = model(tensor).squeeze().cpu().numpy()\n","                features.append(feat)\n","            except Exception as e:\n","                print(f\"Error with {path}: {e}\")\n","                features.append(np.zeros(feature_dim))\n","    return np.array(features)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","model_configs = [\n","    {\n","        'name': 'ResNeXt-101',\n","        'loader': lambda: models.resnext101_32x8d(weights=models.ResNeXt101_32X8D_Weights.DEFAULT),\n","        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n","        'transform': models.ResNet50_Weights.DEFAULT.transforms(),\n","        'feature_dim': 2048\n","    },\n","    {\n","        'name': 'EfficientNet V2',\n","        'loader': lambda: models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.DEFAULT),\n","        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n","        'transform': models.EfficientNet_V2_M_Weights.DEFAULT.transforms(),\n","        'feature_dim': 1280\n","    },\n","    {\n","        'name': 'ConvNeXt',\n","        'loader': lambda: models.convnext_base(weights=models.ConvNeXt_Base_Weights.DEFAULT),\n","        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n","        'transform': models.ConvNeXt_Base_Weights.DEFAULT.transforms(),\n","        'feature_dim': 1024\n","    },\n","    {\n","        'name': 'ViT',\n","        'loader': lambda: models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT),\n","        'modifier': lambda m: (setattr(m.heads, 'head', torch.nn.Identity()), m)[1],\n","        'transform': models.ViT_B_16_Weights.DEFAULT.transforms(),\n","        'feature_dim': 768\n","    },\n","    {\n","        'name': 'Swin Transformer',\n","        'loader': lambda: models.swin_v2_b(weights=models.Swin_V2_B_Weights.DEFAULT),\n","        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n","        'transform': models.Swin_B_Weights.DEFAULT.transforms(),\n","        'feature_dim': 1024\n","    },\n","    {\n","        'name': 'DINOv2',\n","        'loader': lambda: torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14'),\n","        'modifier': lambda m: m,  # No modification needed\n","        'transform': transforms.Compose([\n","            transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ]),\n","        'feature_dim': 768\n","    }\n","]\n","\n","loaded_models = []\n","for config in model_configs:\n","    model = config['loader']()\n","    model = config['modifier'](model)\n","    model = model.to(device)\n","    model.eval()\n","    loaded_models.append({\n","        'model': model,\n","        'name': config['name'],\n","        'transform': config['transform'],\n","        'feature_dim': config['feature_dim']\n","    })\n","\n","# Extract features for all models on train set\n","all_features = {}\n","for model_info in loaded_models:\n","    features = extract_features(\n","        image_paths_dict['train'],\n","        model_info['model'],\n","        model_info['feature_dim'],\n","        model_info['transform'],\n","        model_info['name']\n","    )\n","    all_features[model_info['name']] = features\n","    print(f\"Feature matrix shape - {model_info['name']}: {features.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsJu3KLAk4ji","executionInfo":{"status":"ok","timestamp":1766959809005,"user_tz":360,"elapsed":868575,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"647a9166-2b54-433c-ea34-caf4050762b1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n","xFormers is not available (SwiGLU)\n","xFormers is not available (Attention)\n","xFormers is not available (Block)\n","Extracting features - ResNeXt-101: 100%|██████████| 4614/4614 [02:16<00:00, 33.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - ResNeXt-101: (4614, 2048)\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features - EfficientNet V2: 100%|██████████| 4614/4614 [03:09<00:00, 24.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - EfficientNet V2: (4614, 1280)\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features - ConvNeXt: 100%|██████████| 4614/4614 [01:34<00:00, 49.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - ConvNeXt: (4614, 1024)\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features - ViT: 100%|██████████| 4614/4614 [01:28<00:00, 52.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - ViT: (4614, 768)\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features - Swin Transformer: 100%|██████████| 4614/4614 [03:29<00:00, 21.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - Swin Transformer: (4614, 1024)\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features - DINOv2: 100%|██████████| 4614/4614 [02:05<00:00, 36.64it/s]"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - DINOv2: (4614, 768)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Training the KNN Classifiers\n","\n","KNN is trained using a simple distance-based rule:\n","- Each image is classified based on the majority vote of its *k* nearest neighbors in the feature space.\n","- We use `k=5` neighbors for this experiment.\n","\n","After training, we compute accuracy."],"metadata":{"id":"5jJde8topGt6"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import (confusion_matrix, accuracy_score, f1_score, precision_score, recall_score)\n","\n","RESULTS_PATH = os.path.join(HOME, \"runs\", \"classify\")\n","os.makedirs(RESULTS_PATH, exist_ok=True)\n","\n","encoder = LabelEncoder()\n","y_train = encoder.fit_transform(labels_dict['train'])\n","\n","# Train KNN classifiers for all models\n","trained_models = {}\n","for model_name, features in all_features.items():\n","    knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n","    knn.fit(features, y_train)\n","\n","    trained_models[model_name] = knn\n","    print(f\"Trained KNN for {model_name}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ksSk59glBFq","executionInfo":{"status":"ok","timestamp":1766959810165,"user_tz":360,"elapsed":1151,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"450ad2b3-7688-4c69-fcff-d929217cd671"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Trained KNN for ResNeXt-101\n","Trained KNN for EfficientNet V2\n","Trained KNN for ConvNeXt\n","Trained KNN for ViT\n","Trained KNN for Swin Transformer\n","Trained KNN for DINOv2\n"]}]},{"cell_type":"markdown","source":["## Metrics"],"metadata":{"id":"mcG26nDN9Q9N"}},{"cell_type":"code","source":["def evaluate_model(X, y, split_name, knn, model_name):\n","    split_dir = os.path.join(RESULTS_PATH, split_name)\n","    os.makedirs(split_dir, exist_ok=True)\n","    y_pred = knn.predict(X)\n","\n","    # We need pos_label=0 because the positive class is at 0.\n","    # Kinda weird, but that's how I implemented the dataset parsing and I realized the situation later on, sorry.\n","    accuracy = accuracy_score(y, y_pred)\n","    precision = precision_score(y, y_pred, pos_label=0)\n","    recall = recall_score(y, y_pred, pos_label=0)\n","    f1 = f1_score(y, y_pred, pos_label=0)\n","    conf_matrix = confusion_matrix(y, y_pred)\n","\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1,\n","        'confusion_matrix': conf_matrix\n","    }"],"metadata":{"id":"ViamjOWg9Sea","executionInfo":{"status":"ok","timestamp":1766959810174,"user_tz":360,"elapsed":5,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## Metrics on validation set"],"metadata":{"id":"PTkeOOmipW6i"}},{"cell_type":"code","source":["# Extract features for validation set\n","all_valid_features = {}\n","for model_info in loaded_models:\n","    features = extract_features(\n","        image_paths_dict['valid'],\n","        model_info['model'],\n","        model_info['feature_dim'],\n","        model_info['transform'],\n","        model_info['name']\n","    )\n","    all_valid_features[model_info['name']] = features\n","\n","y_valid = encoder.transform(labels_dict['valid'])\n","\n","# Evaluate all models on valid set\n","for model_name, knn in trained_models.items():\n","  results = evaluate_model(all_valid_features[model_name], y_valid, \"valid\", knn, model_name)\n","  print(f\"{model_name} - Valid Set:\")\n","  print(f\"  Accuracy:  {results['accuracy']:.4f}\")\n","  print(f\"  Precision: {results['precision']:.4f}\")\n","  print(f\"  Recall:    {results['recall']:.4f}\")\n","  print(f\"  F1 Score:  {results['f1_score']:.4f}\")\n","  print(f\"  Confusion Matrix:\")\n","  print(f\"  {results['confusion_matrix']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Gqv0MJIlDSC","outputId":"8d3f4218-ce75-42c9-a5b1-638d6b4ec139","executionInfo":{"status":"ok","timestamp":1766959887850,"user_tz":360,"elapsed":77669,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Extracting features - ResNeXt-101: 100%|██████████| 440/440 [00:12<00:00, 36.66it/s]\n","Extracting features - EfficientNet V2: 100%|██████████| 440/440 [00:16<00:00, 26.17it/s]\n","Extracting features - ConvNeXt: 100%|██████████| 440/440 [00:08<00:00, 51.14it/s]\n","Extracting features - ViT: 100%|██████████| 440/440 [00:08<00:00, 53.19it/s]\n","Extracting features - Swin Transformer: 100%|██████████| 440/440 [00:19<00:00, 22.93it/s]\n","Extracting features - DINOv2: 100%|██████████| 440/440 [00:11<00:00, 37.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ResNeXt-101 - Valid Set:\n","  Accuracy:  0.8977\n","  Precision: 0.8893\n","  Recall:    0.9567\n","  F1 Score:  0.9217\n","  Confusion Matrix:\n","  [[265  12]\n"," [ 33 130]]\n","EfficientNet V2 - Valid Set:\n","  Accuracy:  0.8864\n","  Precision: 0.8796\n","  Recall:    0.9495\n","  F1 Score:  0.9132\n","  Confusion Matrix:\n","  [[263  14]\n"," [ 36 127]]\n","ConvNeXt - Valid Set:\n","  Accuracy:  0.8864\n","  Precision: 0.9039\n","  Recall:    0.9170\n","  F1 Score:  0.9104\n","  Confusion Matrix:\n","  [[254  23]\n"," [ 27 136]]\n","ViT - Valid Set:\n","  Accuracy:  0.8955\n","  Precision: 0.8942\n","  Recall:    0.9458\n","  F1 Score:  0.9193\n","  Confusion Matrix:\n","  [[262  15]\n"," [ 31 132]]\n","Swin Transformer - Valid Set:\n","  Accuracy:  0.8932\n","  Precision: 0.9049\n","  Recall:    0.9278\n","  F1 Score:  0.9162\n","  Confusion Matrix:\n","  [[257  20]\n"," [ 27 136]]\n","DINOv2 - Valid Set:\n","  Accuracy:  0.9250\n","  Precision: 0.9296\n","  Recall:    0.9531\n","  F1 Score:  0.9412\n","  Confusion Matrix:\n","  [[264  13]\n"," [ 20 143]]\n"]}]},{"cell_type":"markdown","source":["## Metrics on test set\n","\n","Don't forget we are trying some models, so that's why we are listing multiple metrics."],"metadata":{"id":"WRwWXMjO8yBe"}},{"cell_type":"code","source":["all_test_features = {}\n","for model_info in loaded_models:\n","    features = extract_features(\n","        image_paths_dict['test'],\n","        model_info['model'],\n","        model_info['feature_dim'],\n","        model_info['transform'],\n","        model_info['name']\n","    )\n","    all_test_features[model_info['name']] = features\n","\n","y_test = encoder.transform(labels_dict['test'])\n","\n","# Evaluate all models on test set\n","for model_name, knn in trained_models.items():\n","    results = evaluate_model(all_test_features[model_name], y_test, \"test\", knn, model_name)\n","    print(f\"{model_name} - Test Set:\")\n","    print(f\"  Accuracy:  {results['accuracy']:.4f}\")\n","    print(f\"  Precision: {results['precision']:.4f}\")\n","    print(f\"  Recall:    {results['recall']:.4f}\")\n","    print(f\"  F1 Score:  {results['f1_score']:.4f}\")\n","    print(f\"  Confusion Matrix:\")\n","    print(f\"  {results['confusion_matrix']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_juqzFfV7pVH","executionInfo":{"status":"ok","timestamp":1766959927261,"user_tz":360,"elapsed":39408,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"538a6052-e332-4fe2-f9b1-cc066e9f6e64"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Extracting features - ResNeXt-101: 100%|██████████| 218/218 [00:06<00:00, 35.19it/s]\n","Extracting features - EfficientNet V2: 100%|██████████| 218/218 [00:08<00:00, 26.23it/s]\n","Extracting features - ConvNeXt: 100%|██████████| 218/218 [00:04<00:00, 48.68it/s]\n","Extracting features - ViT: 100%|██████████| 218/218 [00:04<00:00, 53.71it/s]\n","Extracting features - Swin Transformer: 100%|██████████| 218/218 [00:09<00:00, 22.23it/s]\n","Extracting features - DINOv2: 100%|██████████| 218/218 [00:05<00:00, 38.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ResNeXt-101 - Test Set:\n","  Accuracy:  0.8761\n","  Precision: 0.8562\n","  Recall:    0.9542\n","  F1 Score:  0.9025\n","  Confusion Matrix:\n","  [[125   6]\n"," [ 21  66]]\n","EfficientNet V2 - Test Set:\n","  Accuracy:  0.9083\n","  Precision: 0.8993\n","  Recall:    0.9542\n","  F1 Score:  0.9259\n","  Confusion Matrix:\n","  [[125   6]\n"," [ 14  73]]\n","ConvNeXt - Test Set:\n","  Accuracy:  0.8761\n","  Precision: 0.8881\n","  Recall:    0.9084\n","  F1 Score:  0.8981\n","  Confusion Matrix:\n","  [[119  12]\n"," [ 15  72]]\n","ViT - Test Set:\n","  Accuracy:  0.8899\n","  Precision: 0.8905\n","  Recall:    0.9313\n","  F1 Score:  0.9104\n","  Confusion Matrix:\n","  [[122   9]\n"," [ 15  72]]\n","Swin Transformer - Test Set:\n","  Accuracy:  0.8807\n","  Precision: 0.9070\n","  Recall:    0.8931\n","  F1 Score:  0.9000\n","  Confusion Matrix:\n","  [[117  14]\n"," [ 12  75]]\n","DINOv2 - Test Set:\n","  Accuracy:  0.9128\n","  Precision: 0.9000\n","  Recall:    0.9618\n","  F1 Score:  0.9299\n","  Confusion Matrix:\n","  [[126   5]\n"," [ 14  73]]\n"]}]}]}