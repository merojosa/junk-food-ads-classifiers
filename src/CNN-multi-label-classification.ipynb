{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvlC2Rnni7oi"
      },
      "source": [
        "# Junk Food Multi-label Classification with KNN\n",
        "\n",
        "This notebook implements a **CNN** model for image classification from a **COCO JSON dataset**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcbOFYLzjV6B"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Make sure you have access to GPU. In case of any problems, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, click `Save` and try again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjlhPWyOjWsp",
        "outputId": "76a6e9dd-6398-4cee-c6f6-269582f5f4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 12 20:22:19 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H3H-YHDkdqz",
        "outputId": "486b250a-41d6-4c24-a85d-ba2d308981ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe-7ZMI6kebP",
        "outputId": "902af7ba-6132-4999-e827-9efb9c9ed43f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p {HOME}/datasets\n",
        "%cd {HOME}/datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH1xxtULn2xV"
      },
      "source": [
        "## Install packages using pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xvTEqLaOkjPL",
        "outputId": "e171b929-59e8-402c-f3c5-16b3ee03a0fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow==1.2.11\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: tensorflow==2.19.0 in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2026.1.4)\n",
            "Collecting idna==3.7 (from roboflow==1.2.11)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow==1.2.11)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow==1.2.11)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow==1.2.11)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.0.0)\n",
            "Collecting filetype (from roboflow==1.2.11)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow==1.2.11) (3.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (4.61.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (3.3.1)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow==1.2.11 tensorflow==2.19.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CCGi6EWnsMN"
      },
      "source": [
        "## Download dataset from Roboflow\n",
        "\n",
        "Don't forget to change the `API_KEY` with your dataset key.\n",
        "\n",
        "We replicate your original dataset setup. Even though the dataset is labeled for object detection, we’ll use the full image classification approach with KNN. Labels will be derived from the most frequent class per image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM4b5M09kj3D",
        "outputId": "6b9dba28-2fb4-4b7d-f035-3afadc8e79a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Junk-Food-Detection-10 to coco:: 100%|██████████| 293482/293482 [00:03<00:00, 92789.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Junk-Food-Detection-10 in coco:: 100%|██████████| 5280/5280 [00:00<00:00, 5775.64it/s]\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "from google.colab import userdata\n",
        "\n",
        "rf = Roboflow(api_key=userdata.get('ROBOFLOW_API_KEY'))\n",
        "project = rf.workspace(userdata.get('ROBOFLOW_WORKSPACE_ID')).project(userdata.get('ROBOFLOW_PROJECT_ID'))\n",
        "version = project.version(userdata.get('ROBOFLOW_DATASET_VERSION'))\n",
        "dataset = version.download(\"coco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71fxf9Tsktei",
        "outputId": "7fd50f99-21be-427a-f604-7f8ce3baa06b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM3porFAovJj"
      },
      "source": [
        "## Convert COCO detection dataset to EfficientNetV2 multi-label classification\n",
        "\n",
        "For labeling, we use the 7 classes from the COCO JSON dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QaNLLGlie3V",
        "outputId": "4ca160c7-4cac-4376-8726-38b2b5f06468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded train set: 4614 images, 7 classes\n",
            "Labels shape: (4614, 7)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List, Dict\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def load_coco_annotations(json_path: str) -> Tuple[Dict, List, Dict]:\n",
        "    with open(json_path, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    # Create mappings\n",
        "    images_dict = {img['id']: img for img in coco_data['images']}\n",
        "\n",
        "    # Filter out \"junk-food\" category\n",
        "    categories = [cat for cat in coco_data['categories'] if cat['name'] != 'junk-food']\n",
        "\n",
        "    # Get IDs of categories to keep\n",
        "    valid_category_ids = {cat['id'] for cat in categories}\n",
        "\n",
        "    # Group annotations by image_id, filtering out junk-food annotations\n",
        "    annotations_by_image = {}\n",
        "    for ann in coco_data['annotations']:\n",
        "        # Skip if this annotation is for junk-food\n",
        "        if ann['category_id'] not in valid_category_ids:\n",
        "            continue\n",
        "\n",
        "        image_id = ann['image_id']\n",
        "        if image_id not in annotations_by_image:\n",
        "            annotations_by_image[image_id] = []\n",
        "        annotations_by_image[image_id].append(ann['category_id'])\n",
        "\n",
        "    return annotations_by_image, categories, images_dict\n",
        "\n",
        "\n",
        "def create_label_mapping(categories: List[Dict]) -> Tuple[Dict, Dict, int]:\n",
        "    \"\"\"\n",
        "    Create category ID to index mapping for multi-label classification.\n",
        "    \"\"\"\n",
        "    # Sort categories by ID for consistency\n",
        "    sorted_categories = sorted(categories, key=lambda x: x['id'])\n",
        "\n",
        "    cat_id_to_idx = {cat['id']: idx for idx, cat in enumerate(sorted_categories)}\n",
        "    idx_to_cat_id = {idx: cat['id'] for idx, cat in enumerate(sorted_categories)}\n",
        "    num_classes = len(categories)\n",
        "\n",
        "    return cat_id_to_idx, idx_to_cat_id, num_classes\n",
        "\n",
        "\n",
        "def transform_coco_to_multilabel(\n",
        "    dataset_location: str,\n",
        "    image_size: Tuple[int, int],\n",
        "    subset: str = 'train',\n",
        ") -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
        "    \"\"\"\n",
        "    Transform COCO JSON dataset into format for EfficientNetV2 multi-label classification.\n",
        "    \"\"\"\n",
        "    # Construct paths\n",
        "    subset_path = os.path.join(dataset_location, subset)\n",
        "    json_path = os.path.join(subset_path, '_annotations.coco.json')\n",
        "\n",
        "    if not os.path.exists(json_path):\n",
        "        raise FileNotFoundError(f\"Annotations file not found at {json_path}\")\n",
        "\n",
        "    # Load COCO annotations\n",
        "    annotations_by_image, categories, images_dict = load_coco_annotations(json_path)\n",
        "\n",
        "    # Create label mappings\n",
        "    cat_id_to_idx, idx_to_cat_id, num_classes = create_label_mapping(categories)\n",
        "\n",
        "    # Prepare lists for data\n",
        "    image_paths = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Process each image\n",
        "    for image_id, image_info in images_dict.items():\n",
        "        # Get image path\n",
        "        image_filename = image_info['file_name']\n",
        "        image_path = os.path.join(subset_path, image_filename)\n",
        "\n",
        "        # Check if image exists\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Warning: Image not found: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        # Create multi-hot encoded label\n",
        "        label_vector = np.zeros(num_classes, dtype=np.float32)\n",
        "\n",
        "        # Get annotations for this image\n",
        "        if image_id in annotations_by_image:\n",
        "            category_ids = annotations_by_image[image_id]\n",
        "            for cat_id in category_ids:\n",
        "                if cat_id in cat_id_to_idx:\n",
        "                    idx = cat_id_to_idx[cat_id]\n",
        "                    label_vector[idx] = 1.0\n",
        "\n",
        "        image_paths.append(image_path)\n",
        "        labels_list.append(label_vector)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    image_paths = np.array(image_paths)\n",
        "    labels = np.array(labels_list)\n",
        "\n",
        "    # Create metadata dictionary\n",
        "    metadata = {\n",
        "        'num_classes': num_classes,\n",
        "        'cat_id_to_idx': cat_id_to_idx,\n",
        "        'idx_to_cat_id': idx_to_cat_id,\n",
        "        'categories': categories,\n",
        "        'image_size': image_size,\n",
        "        'subset': subset,\n",
        "        'num_samples': len(image_paths)\n",
        "    }\n",
        "\n",
        "    print(f\"Loaded {subset} set: {len(image_paths)} images, {num_classes} classes\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "    return image_paths, labels, metadata\n",
        "\n",
        "\n",
        "def create_tf_dataset(\n",
        "    image_paths: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    metadata: Dict,\n",
        "    batch_size: int = 32\n",
        ") -> tf.data.Dataset:\n",
        "    \"\"\"\n",
        "    Create a TensorFlow dataset from image paths and labels for EfficientNetV2.\n",
        "    \"\"\"\n",
        "    image_size = metadata['image_size']\n",
        "\n",
        "    def load_and_preprocess_image(image_path, label):\n",
        "        # Read image\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "        # Resize\n",
        "        image = tf.image.resize(image, image_size)\n",
        "\n",
        "        # Preprocess for EfficientNet (scales to [-1, 1])\n",
        "        image = tf.keras.applications.efficientnet_v2.preprocess_input(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "train_image_paths, train_labels, train_metadata = transform_coco_to_multilabel(\n",
        "    dataset.location,\n",
        "    subset='train',\n",
        "    image_size=(640, 640)\n",
        ")\n",
        "\n",
        "train_dataset = create_tf_dataset(\n",
        "    train_image_paths,\n",
        "    train_labels,\n",
        "    train_metadata,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "odINh0pF3tox"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXuF9EPPVXDk"
      },
      "source": [
        "## Train multi-label classification EfficientNetV2 model with dataset\n",
        "\n",
        "We train the EfficientNetV2 model with early stopping, a model checkpoint (to save the best resultant model), and display the required metrics for our evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg8VP7hQV2Ux",
        "outputId": "4e25dc25-4829-41a9-84b3-8ef5f2b337e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded valid set: 440 images, 7 classes\n",
            "Labels shape: (440, 7)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/50\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635ms/step - auc: 0.6123 - loss: 0.4518 - macro_f1: 0.1558 - micro_f1: 0.2770 - subset_accuracy: 0.3100\n",
            "Epoch 1: val_loss improved from inf to 0.25082, saving model to best_model.keras\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 908ms/step - auc: 0.6130 - loss: 0.4510 - macro_f1: 0.1558 - micro_f1: 0.2775 - subset_accuracy: 0.3106 - val_auc: 0.8747 - val_loss: 0.2508 - val_macro_f1: 0.3638 - val_micro_f1: 0.5559 - val_subset_accuracy: 0.5068\n",
            "Epoch 2/50\n",
            "\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - auc: 0.9215 - loss: 0.2172 - macro_f1: 0.4420 - micro_f1: 0.6147 - subset_accuracy: 0.5486\n",
            "Epoch 2: val_loss improved from 0.25082 to 0.15852, saving model to best_model.keras\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 132ms/step - auc: 0.9217 - loss: 0.2168 - macro_f1: 0.4437 - micro_f1: 0.6157 - subset_accuracy: 0.5495 - val_auc: 0.9574 - val_loss: 0.1585 - val_macro_f1: 0.6831 - val_micro_f1: 0.7556 - val_subset_accuracy: 0.6750\n",
            "Epoch 3/50\n",
            "\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9818 - loss: 0.1197 - macro_f1: 0.7986 - micro_f1: 0.8415 - subset_accuracy: 0.7579\n",
            "Epoch 3: val_loss improved from 0.15852 to 0.12544, saving model to best_model.keras\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 132ms/step - auc: 0.9819 - loss: 0.1195 - macro_f1: 0.7990 - micro_f1: 0.8418 - subset_accuracy: 0.7584 - val_auc: 0.9667 - val_loss: 0.1254 - val_macro_f1: 0.8064 - val_micro_f1: 0.8374 - val_subset_accuracy: 0.7568\n",
            "Epoch 4/50\n",
            "\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9950 - loss: 0.0693 - macro_f1: 0.9124 - micro_f1: 0.9285 - subset_accuracy: 0.8749\n",
            "Epoch 4: val_loss improved from 0.12544 to 0.11768, saving model to best_model.keras\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 132ms/step - auc: 0.9951 - loss: 0.0692 - macro_f1: 0.9126 - micro_f1: 0.9287 - subset_accuracy: 0.8752 - val_auc: 0.9706 - val_loss: 0.1177 - val_macro_f1: 0.8269 - val_micro_f1: 0.8505 - val_subset_accuracy: 0.7727\n",
            "Epoch 5/50\n",
            "\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9988 - loss: 0.0398 - macro_f1: 0.9552 - micro_f1: 0.9620 - subset_accuracy: 0.9293\n",
            "Epoch 5: val_loss improved from 0.11768 to 0.11767, saving model to best_model.keras\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 132ms/step - auc: 0.9988 - loss: 0.0397 - macro_f1: 0.9553 - micro_f1: 0.9621 - subset_accuracy: 0.9295 - val_auc: 0.9688 - val_loss: 0.1177 - val_macro_f1: 0.8495 - val_micro_f1: 0.8650 - val_subset_accuracy: 0.7932\n",
            "Epoch 6/50\n",
            "\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9997 - loss: 0.0259 - macro_f1: 0.9764 - micro_f1: 0.9787 - subset_accuracy: 0.9605\n",
            "Epoch 6: val_loss did not improve from 0.11767\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 124ms/step - auc: 0.9997 - loss: 0.0258 - macro_f1: 0.9764 - micro_f1: 0.9787 - subset_accuracy: 0.9606 - val_auc: 0.9614 - val_loss: 0.1397 - val_macro_f1: 0.8340 - val_micro_f1: 0.8478 - val_subset_accuracy: 0.7773\n",
            "Epoch 7/50\n",
            "\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9998 - loss: 0.0191 - macro_f1: 0.9884 - micro_f1: 0.9881 - subset_accuracy: 0.9769\n",
            "Epoch 7: val_loss did not improve from 0.11767\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 123ms/step - auc: 0.9998 - loss: 0.0190 - macro_f1: 0.9884 - micro_f1: 0.9881 - subset_accuracy: 0.9770 - val_auc: 0.9647 - val_loss: 0.1185 - val_macro_f1: 0.8550 - val_micro_f1: 0.8707 - val_subset_accuracy: 0.7909\n",
            "Epoch 8/50\n",
            "\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - auc: 0.9999 - loss: 0.0135 - macro_f1: 0.9926 - micro_f1: 0.9940 - subset_accuracy: 0.9891\n",
            "Epoch 8: val_loss did not improve from 0.11767\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 124ms/step - auc: 0.9999 - loss: 0.0135 - macro_f1: 0.9926 - micro_f1: 0.9940 - subset_accuracy: 0.9891 - val_auc: 0.9623 - val_loss: 0.1255 - val_macro_f1: 0.8549 - val_micro_f1: 0.8678 - val_subset_accuracy: 0.7932\n",
            "Epoch 9/50\n",
            "\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 1.0000 - loss: 0.0110 - macro_f1: 0.9926 - micro_f1: 0.9926 - subset_accuracy: 0.9855\n",
            "Epoch 9: val_loss did not improve from 0.11767\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 123ms/step - auc: 1.0000 - loss: 0.0110 - macro_f1: 0.9926 - micro_f1: 0.9927 - subset_accuracy: 0.9855 - val_auc: 0.9572 - val_loss: 0.1378 - val_macro_f1: 0.8522 - val_micro_f1: 0.8711 - val_subset_accuracy: 0.8023\n",
            "Epoch 10/50\n",
            "\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 1.0000 - loss: 0.0092 - macro_f1: 0.9945 - micro_f1: 0.9944 - subset_accuracy: 0.9893\n",
            "Epoch 10: val_loss did not improve from 0.11767\n",
            "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 123ms/step - auc: 1.0000 - loss: 0.0092 - macro_f1: 0.9945 - micro_f1: 0.9944 - subset_accuracy: 0.9893 - val_auc: 0.9564 - val_loss: 0.1422 - val_macro_f1: 0.8436 - val_micro_f1: 0.8592 - val_subset_accuracy: 0.7818\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "valid_image_paths, valid_labels_train, valid_metadata = transform_coco_to_multilabel(\n",
        "    dataset.location,\n",
        "    subset='valid',\n",
        "    image_size=(640, 640)\n",
        ")\n",
        "\n",
        "valid_dataset = create_tf_dataset(\n",
        "    valid_image_paths,\n",
        "    valid_labels_train,\n",
        "    valid_metadata,\n",
        ")\n",
        "\n",
        "# Custom F1 Score metric (this is Micro F1)\n",
        "class MicroF1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='micro_f1', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        p = self.precision.result()\n",
        "        r = self.recall.result()\n",
        "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.precision.reset_state()\n",
        "        self.recall.reset_state()\n",
        "\n",
        "# Custom Macro F1 Score metric\n",
        "class MacroF1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, num_classes, name='macro_f1', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.precisions = [tf.keras.metrics.Precision() for _ in range(num_classes)]\n",
        "        self.recalls = [tf.keras.metrics.Recall() for _ in range(num_classes)]\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        for i in range(self.num_classes):\n",
        "            self.precisions[i].update_state(y_true[:, i], y_pred[:, i], sample_weight)\n",
        "            self.recalls[i].update_state(y_true[:, i], y_pred[:, i], sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        f1_scores = []\n",
        "        for i in range(self.num_classes):\n",
        "            p = self.precisions[i].result()\n",
        "            r = self.recalls[i].result()\n",
        "            f1 = 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
        "            f1_scores.append(f1)\n",
        "        return tf.reduce_mean(f1_scores)\n",
        "\n",
        "    def reset_state(self):\n",
        "        for i in range(self.num_classes):\n",
        "            self.precisions[i].reset_state()\n",
        "            self.recalls[i].reset_state()\n",
        "\n",
        "# Custom Subset Accuracy metric\n",
        "class SubsetAccuracy(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='subset_accuracy', threshold=0.5, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.threshold = threshold\n",
        "        self.correct = self.add_weight(name='correct', initializer='zeros')\n",
        "        self.total = self.add_weight(name='total', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred_binary = tf.cast(y_pred >= self.threshold, tf.float32)\n",
        "        exact_matches = tf.reduce_all(tf.equal(y_true, y_pred_binary), axis=1)\n",
        "        self.correct.assign_add(tf.reduce_sum(tf.cast(exact_matches, tf.float32)))\n",
        "        self.total.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.correct / (self.total + tf.keras.backend.epsilon())\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.correct.assign(0.0)\n",
        "        self.total.assign(0.0)\n",
        "\n",
        "# Build EfficientNetV2 multi-label classification model\n",
        "base_model = tf.keras.applications.EfficientNetV2B0(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(640, 640, 3),\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "# Unfreeze base model for fine-tuning\n",
        "base_model.trainable = True\n",
        "\n",
        "# Build model\n",
        "inputs = tf.keras.Input(shape=(640, 640, 3))\n",
        "x = base_model(inputs, training=True)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(train_metadata['num_classes'], activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile with all requested metrics\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        MicroF1Score(name='micro_f1'),\n",
        "        MacroF1Score(num_classes=train_metadata['num_classes'], name='macro_f1'),\n",
        "        tf.keras.metrics.AUC(name='auc', multi_label=True),\n",
        "        SubsetAccuracy(name='subset_accuracy', threshold=0.5)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='best_model.keras',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('efficientnet_multilabel_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83a7g8nff5n5"
      },
      "source": [
        "## Run predictions on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCi2cHdSxFxz",
        "outputId": "bc536e45-04e7-417c-e444-c0f8d62b1778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded test set: 218 images, 7 classes\n",
            "Labels shape: (218, 7)\n",
            "Generating predictions...\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 4s/step\n",
            "\n",
            "==================================================\n",
            "TEST SET METRICS\n",
            "==================================================\n",
            "Subset Accuracy: 0.7890\n",
            "Micro F1:        0.8516\n",
            "Macro F1:        0.8348\n",
            "\n",
            "==================================================\n",
            "F1 SCORE PER CLASS\n",
            "==================================================\n",
            "french_fries: 0.7333\n",
            "fried_chicken: 0.9091\n",
            "hamburger: 0.9655\n",
            "ice_cream: 0.7368\n",
            "junk_food_logo: 0.8875\n",
            "pizza: 0.8889\n",
            "soda: 0.7222\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Load the test dataset\n",
        "test_image_paths, test_labels, test_metadata = transform_coco_to_multilabel(\n",
        "    dataset.location,\n",
        "    subset='test',\n",
        "    image_size=(640, 640)\n",
        ")\n",
        "\n",
        "test_dataset = create_tf_dataset(\n",
        "    test_image_paths,\n",
        "    test_labels,\n",
        "    test_metadata,\n",
        ")\n",
        "\n",
        "# Load the best model (no custom objects needed)\n",
        "best_model = tf.keras.models.load_model('best_model.keras', compile=False)\n",
        "\n",
        "# Generate predictions\n",
        "print(\"Generating predictions...\")\n",
        "y_pred_probs = best_model.predict(test_dataset, verbose=1)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Get true labels\n",
        "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"TEST SET METRICS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "subset_accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
        "micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "print(f\"Micro F1:        {micro_f1:.4f}\")\n",
        "macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "print(f\"Macro F1:        {macro_f1:.4f}\")\n",
        "\n",
        "# F1 score per class\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"F1 SCORE PER CLASS\")\n",
        "print(\"=\" * 50)\n",
        "class_names = [cat['name'] for cat in sorted(test_metadata['categories'], key=lambda x: x['id'])]\n",
        "f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
        "for class_name, f1 in zip(class_names, f1_per_class):\n",
        "    print(f\"{class_name}: {f1:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}