{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Junk Food Classification with KNN\n","\n","This notebook implements a **K-Nearest Neighbors (KNN)** classifier for image classification using a **COCO-style dataset**.  \n","The goal is to compare the performance of KNN against YOLO and CLIP pipelines using the same dataset and consistent evaluation metrics.\n","\n","Running the pipeline is relatively quick, since it uses a pre-trained model (RestNet-50)"],"metadata":{"id":"EvlC2Rnni7oi"}},{"cell_type":"markdown","source":["## Before you start\n","\n","Make sure you have access to GPU. In case of any problems, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, click `Save` and try again."],"metadata":{"id":"gcbOFYLzjV6B"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjlhPWyOjWsp","executionInfo":{"status":"ok","timestamp":1766912430853,"user_tz":360,"elapsed":208,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"b5f5cf0d-a16f-483a-a0ca-260db9edf0aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Dec 28 09:00:30 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   29C    P0             42W /  400W |       0MiB /  40960MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["import os\n","HOME = os.getcwd()\n","print(\"HOME:\", HOME)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5H3H-YHDkdqz","executionInfo":{"status":"ok","timestamp":1766912430867,"user_tz":360,"elapsed":11,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"3605ccc0-e872-401e-cd9f-0b1b797910a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["HOME: /content\n"]}]},{"cell_type":"code","source":["!mkdir -p {HOME}/datasets\n","%cd {HOME}/datasets\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xe-7ZMI6kebP","executionInfo":{"status":"ok","timestamp":1766912430983,"user_tz":360,"elapsed":115,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"1d52976d-848c-400c-e3a4-ce30b44cf67b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/datasets\n"]}]},{"cell_type":"markdown","source":["## Install packages using pip"],"metadata":{"id":"mH1xxtULn2xV"}},{"cell_type":"code","source":["!pip install roboflow==1.2.11 tensorflow==2.19.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xvTEqLaOkjPL","executionInfo":{"status":"ok","timestamp":1766912439583,"user_tz":360,"elapsed":8598,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"891c8ec8-a250-4188-82dd-2c1386a43792","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting roboflow==1.2.11\n","  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: tensorflow==2.19.0 in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2025.11.12)\n","Collecting idna==3.7 (from roboflow==1.2.11)\n","  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.4.9)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (3.10.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.0.2)\n","Collecting opencv-python-headless==4.10.0.84 (from roboflow==1.2.11)\n","  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (11.3.0)\n","Collecting pi-heif<2 (from roboflow==1.2.11)\n","  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n","Collecting pillow-avif-plugin<2 (from roboflow==1.2.11)\n","  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.9.0.post0)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.2.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.32.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.17.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.5.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (4.67.1)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (6.0.3)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.0.0)\n","Collecting filetype (from roboflow==1.2.11)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.9.23)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.7.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.2.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.76.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.15.1)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.18.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow==1.2.11) (3.4.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.10)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (1.3.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (4.61.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (3.2.5)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n","Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m138.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.12.0.88\n","    Uninstalling opencv-python-headless-4.12.0.88:\n","      Successfully uninstalled opencv-python-headless-4.12.0.88\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.11\n","    Uninstalling idna-3.11:\n","      Successfully uninstalled idna-3.11\n","Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"]}]},{"cell_type":"markdown","source":["## Download dataset from Roboflow\n","\n","Don't forget to change the `API_KEY` with your dataset key.\n","\n","We replicate your original dataset setup. Even though the dataset is labeled for object detection, we’ll use the full image classification approach with KNN. Labels will be derived from the most frequent class per image."],"metadata":{"id":"_CCGi6EWnsMN"}},{"cell_type":"code","source":["from roboflow import Roboflow\n","from google.colab import userdata\n","\n","rf = Roboflow(api_key=userdata.get('ROBOFLOW_API_KEY'))\n","project = rf.workspace(userdata.get('ROBOFLOW_WORKSPACE_ID')).project(userdata.get('ROBOFLOW_PROJECT_ID'))\n","version = project.version(userdata.get('ROBOFLOW_DATASET_VERSION'))\n","dataset = version.download(\"coco\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XM4b5M09kj3D","executionInfo":{"status":"ok","timestamp":1766912446722,"user_tz":360,"elapsed":7136,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"7f958832-c617-43e0-e9d2-d0dd280ad35a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading Dataset Version Zip in Junk-Food-Detection-10 to coco:: 100%|██████████| 293482/293482 [00:03<00:00, 94464.04it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n","Extracting Dataset Version Zip to Junk-Food-Detection-10 in coco:: 100%|██████████| 5280/5280 [00:00<00:00, 5705.76it/s]\n"]}]},{"cell_type":"code","source":["%cd {HOME}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71fxf9Tsktei","executionInfo":{"status":"ok","timestamp":1766912446730,"user_tz":360,"elapsed":6,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"191ba99c-e027-4980-ed92-66387b7dab45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"markdown","source":["## Convert COCO detection dataset to EfficientNetV2 multi-label classification\n","\n","We will use a classification model. So, for labeling, we use two classes: junk-food-ad and non-junk-food-ad. Given the fact that the dataset is multiclass, the rule is: if there is at least one bounding box belonging to a particular image, it's junk-food-ad. Otherwise, it's non-junk-food-ad"],"metadata":{"id":"LM3porFAovJj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QaNLLGlie3V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766912456253,"user_tz":360,"elapsed":9514,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"9dfe0a1d-45c8-4381-94e6-5d0d541b686a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded train set: 4614 images, 7 classes\n","Labels shape: (4614, 7)\n"]}],"source":["import json\n","import os\n","import numpy as np\n","from PIL import Image\n","from pathlib import Path\n","from typing import Tuple, List, Dict\n","import tensorflow as tf\n","\n","\n","def load_coco_annotations(json_path: str) -> Tuple[Dict, List, Dict]:\n","    with open(json_path, 'r') as f:\n","        coco_data = json.load(f)\n","\n","    # Create mappings\n","    images_dict = {img['id']: img for img in coco_data['images']}\n","\n","    # Filter out \"junk-food\" category\n","    categories = [cat for cat in coco_data['categories'] if cat['name'] != 'junk-food']\n","\n","    # Get IDs of categories to keep\n","    valid_category_ids = {cat['id'] for cat in categories}\n","\n","    # Group annotations by image_id, filtering out junk-food annotations\n","    annotations_by_image = {}\n","    for ann in coco_data['annotations']:\n","        # Skip if this annotation is for junk-food\n","        if ann['category_id'] not in valid_category_ids:\n","            continue\n","\n","        image_id = ann['image_id']\n","        if image_id not in annotations_by_image:\n","            annotations_by_image[image_id] = []\n","        annotations_by_image[image_id].append(ann['category_id'])\n","\n","    return annotations_by_image, categories, images_dict\n","\n","\n","def create_label_mapping(categories: List[Dict]) -> Tuple[Dict, Dict, int]:\n","    \"\"\"\n","    Create category ID to index mapping for multi-label classification.\n","    \"\"\"\n","    # Sort categories by ID for consistency\n","    sorted_categories = sorted(categories, key=lambda x: x['id'])\n","\n","    cat_id_to_idx = {cat['id']: idx for idx, cat in enumerate(sorted_categories)}\n","    idx_to_cat_id = {idx: cat['id'] for idx, cat in enumerate(sorted_categories)}\n","    num_classes = len(categories)\n","\n","    return cat_id_to_idx, idx_to_cat_id, num_classes\n","\n","\n","def transform_coco_to_multilabel(\n","    dataset_location: str,\n","    image_size: Tuple[int, int],\n","    subset: str = 'train',\n",") -> Tuple[np.ndarray, np.ndarray, Dict]:\n","    \"\"\"\n","    Transform COCO JSON dataset into format for EfficientNetV2 multi-label classification.\n","    \"\"\"\n","    # Construct paths\n","    subset_path = os.path.join(dataset_location, subset)\n","    json_path = os.path.join(subset_path, '_annotations.coco.json')\n","\n","    if not os.path.exists(json_path):\n","        raise FileNotFoundError(f\"Annotations file not found at {json_path}\")\n","\n","    # Load COCO annotations\n","    annotations_by_image, categories, images_dict = load_coco_annotations(json_path)\n","\n","    # Create label mappings\n","    cat_id_to_idx, idx_to_cat_id, num_classes = create_label_mapping(categories)\n","\n","    # Prepare lists for data\n","    image_paths = []\n","    labels_list = []\n","\n","    # Process each image\n","    for image_id, image_info in images_dict.items():\n","        # Get image path\n","        image_filename = image_info['file_name']\n","        image_path = os.path.join(subset_path, image_filename)\n","\n","        # Check if image exists\n","        if not os.path.exists(image_path):\n","            print(f\"Warning: Image not found: {image_path}\")\n","            continue\n","\n","        # Create multi-hot encoded label\n","        label_vector = np.zeros(num_classes, dtype=np.float32)\n","\n","        # Get annotations for this image\n","        if image_id in annotations_by_image:\n","            category_ids = annotations_by_image[image_id]\n","            for cat_id in category_ids:\n","                if cat_id in cat_id_to_idx:\n","                    idx = cat_id_to_idx[cat_id]\n","                    label_vector[idx] = 1.0\n","\n","        image_paths.append(image_path)\n","        labels_list.append(label_vector)\n","\n","    # Convert to numpy arrays\n","    image_paths = np.array(image_paths)\n","    labels = np.array(labels_list)\n","\n","    # Create metadata dictionary\n","    metadata = {\n","        'num_classes': num_classes,\n","        'cat_id_to_idx': cat_id_to_idx,\n","        'idx_to_cat_id': idx_to_cat_id,\n","        'categories': categories,\n","        'image_size': image_size,\n","        'subset': subset,\n","        'num_samples': len(image_paths)\n","    }\n","\n","    print(f\"Loaded {subset} set: {len(image_paths)} images, {num_classes} classes\")\n","    print(f\"Labels shape: {labels.shape}\")\n","\n","    return image_paths, labels, metadata\n","\n","\n","def create_tf_dataset(\n","    image_paths: np.ndarray,\n","    labels: np.ndarray,\n","    metadata: Dict,\n","    batch_size: int = 32\n",") -> tf.data.Dataset:\n","    \"\"\"\n","    Create a TensorFlow dataset from image paths and labels for EfficientNetV2.\n","    \"\"\"\n","    image_size = metadata['image_size']\n","\n","    def load_and_preprocess_image(image_path, label):\n","        # Read image\n","        image = tf.io.read_file(image_path)\n","        image = tf.image.decode_jpeg(image, channels=3)\n","\n","        # Resize\n","        image = tf.image.resize(image, image_size)\n","\n","        # Preprocess for EfficientNet (scales to [-1, 1])\n","        image = tf.keras.applications.efficientnet_v2.preprocess_input(image)\n","\n","        return image, label\n","\n","    # Create dataset\n","    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n","    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n","\n","    return dataset\n","\n","train_image_paths, train_labels, train_metadata = transform_coco_to_multilabel(\n","    dataset.location,\n","    subset='train',\n","    image_size=(640, 640)\n",")\n","\n","train_dataset = create_tf_dataset(\n","    train_image_paths,\n","    train_labels,\n","    train_metadata,\n",")"]},{"cell_type":"markdown","source":["## Train multi-label classification EfficientNetV2 model with dataset"],"metadata":{"id":"CXuF9EPPVXDk"}},{"cell_type":"code","source":["from tensorflow.keras import layers, Model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import tensorflow as tf\n","\n","valid_image_paths, valid_labels_train, valid_metadata = transform_coco_to_multilabel(\n","    dataset.location,\n","    subset='valid',\n","    image_size=(640, 640)\n",")\n","\n","valid_dataset = create_tf_dataset(\n","    valid_image_paths,\n","    valid_labels_train,\n","    valid_metadata,\n",")\n","\n","# Custom F1 Score metric (this is Micro F1)\n","class MicroF1Score(tf.keras.metrics.Metric):\n","    def __init__(self, name='micro_f1', **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.precision = tf.keras.metrics.Precision()\n","        self.recall = tf.keras.metrics.Recall()\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        self.precision.update_state(y_true, y_pred, sample_weight)\n","        self.recall.update_state(y_true, y_pred, sample_weight)\n","\n","    def result(self):\n","        p = self.precision.result()\n","        r = self.recall.result()\n","        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n","\n","    def reset_state(self):\n","        self.precision.reset_state()\n","        self.recall.reset_state()\n","\n","# Custom Macro F1 Score metric\n","class MacroF1Score(tf.keras.metrics.Metric):\n","    def __init__(self, num_classes, name='macro_f1', **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.num_classes = num_classes\n","        self.precisions = [tf.keras.metrics.Precision() for _ in range(num_classes)]\n","        self.recalls = [tf.keras.metrics.Recall() for _ in range(num_classes)]\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        for i in range(self.num_classes):\n","            self.precisions[i].update_state(y_true[:, i], y_pred[:, i], sample_weight)\n","            self.recalls[i].update_state(y_true[:, i], y_pred[:, i], sample_weight)\n","\n","    def result(self):\n","        f1_scores = []\n","        for i in range(self.num_classes):\n","            p = self.precisions[i].result()\n","            r = self.recalls[i].result()\n","            f1 = 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n","            f1_scores.append(f1)\n","        return tf.reduce_mean(f1_scores)\n","\n","    def reset_state(self):\n","        for i in range(self.num_classes):\n","            self.precisions[i].reset_state()\n","            self.recalls[i].reset_state()\n","\n","# Custom Subset Accuracy metric\n","class SubsetAccuracy(tf.keras.metrics.Metric):\n","    def __init__(self, name='subset_accuracy', threshold=0.5, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.threshold = threshold\n","        self.correct = self.add_weight(name='correct', initializer='zeros')\n","        self.total = self.add_weight(name='total', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_pred_binary = tf.cast(y_pred >= self.threshold, tf.float32)\n","        exact_matches = tf.reduce_all(tf.equal(y_true, y_pred_binary), axis=1)\n","        self.correct.assign_add(tf.reduce_sum(tf.cast(exact_matches, tf.float32)))\n","        self.total.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n","\n","    def result(self):\n","        return self.correct / (self.total + tf.keras.backend.epsilon())\n","\n","    def reset_state(self):\n","        self.correct.assign(0.0)\n","        self.total.assign(0.0)\n","\n","# Build EfficientNetV2 multi-label classification model\n","base_model = tf.keras.applications.EfficientNetV2B0(\n","    include_top=False,\n","    weights='imagenet',\n","    input_shape=(640, 640, 3),\n","    pooling='avg'\n",")\n","\n","# Unfreeze base model for fine-tuning\n","base_model.trainable = True\n","\n","# Build model\n","inputs = tf.keras.Input(shape=(640, 640, 3))\n","x = base_model(inputs, training=True)\n","x = layers.Dropout(0.2)(x)\n","outputs = layers.Dense(train_metadata['num_classes'], activation='sigmoid')(x)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","\n","# Compile with all requested metrics\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n","    loss='binary_crossentropy',\n","    metrics=[\n","        MicroF1Score(name='micro_f1'),\n","        MacroF1Score(num_classes=train_metadata['num_classes'], name='macro_f1'),\n","        tf.keras.metrics.AUC(name='auc', multi_label=True),\n","        SubsetAccuracy(name='subset_accuracy', threshold=0.5)\n","    ]\n",")\n","\n","# Callbacks\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","model_checkpoint = ModelCheckpoint(\n","    filepath='best_model.keras',\n","    monitor='val_loss',\n","    save_best_only=True,\n","    verbose=1\n",")\n","\n","# Train the model\n","history = model.fit(\n","    train_dataset,\n","    validation_data=valid_dataset,\n","    epochs=50,\n","    callbacks=[early_stopping, model_checkpoint],\n","    verbose=1\n",")\n","\n","# Save the model\n","model.save('efficientnet_multilabel_model.keras')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mg8VP7hQV2Ux","outputId":"fcbe8b21-adf5-4ede-b54b-b043ebd8e05a","executionInfo":{"status":"ok","timestamp":1766912891940,"user_tz":360,"elapsed":435682,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded valid set: 440 images, 7 classes\n","Labels shape: (440, 7)\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n","\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Epoch 1/50\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638ms/step - auc: 0.6145 - loss: 0.4515 - macro_f1: 0.1548 - micro_f1: 0.2773 - subset_accuracy: 0.3164\n","Epoch 1: val_loss improved from inf to 0.25054, saving model to best_model.keras\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 916ms/step - auc: 0.6152 - loss: 0.4508 - macro_f1: 0.1548 - micro_f1: 0.2778 - subset_accuracy: 0.3170 - val_auc: 0.8813 - val_loss: 0.2505 - val_macro_f1: 0.3355 - val_micro_f1: 0.5521 - val_subset_accuracy: 0.5045\n","Epoch 2/50\n","\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9180 - loss: 0.2205 - macro_f1: 0.4039 - micro_f1: 0.6062 - subset_accuracy: 0.5323\n","Epoch 2: val_loss improved from 0.25054 to 0.16310, saving model to best_model.keras\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 132ms/step - auc: 0.9183 - loss: 0.2201 - macro_f1: 0.4055 - micro_f1: 0.6071 - subset_accuracy: 0.5332 - val_auc: 0.9561 - val_loss: 0.1631 - val_macro_f1: 0.6550 - val_micro_f1: 0.7394 - val_subset_accuracy: 0.6727\n","Epoch 3/50\n","\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - auc: 0.9805 - loss: 0.1231 - macro_f1: 0.7731 - micro_f1: 0.8315 - subset_accuracy: 0.7435\n","Epoch 3: val_loss improved from 0.16310 to 0.12666, saving model to best_model.keras\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 132ms/step - auc: 0.9806 - loss: 0.1229 - macro_f1: 0.7737 - micro_f1: 0.8319 - subset_accuracy: 0.7441 - val_auc: 0.9683 - val_loss: 0.1267 - val_macro_f1: 0.8045 - val_micro_f1: 0.8350 - val_subset_accuracy: 0.7523\n","Epoch 4/50\n","\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9958 - loss: 0.0666 - macro_f1: 0.9095 - micro_f1: 0.9314 - subset_accuracy: 0.8800\n","Epoch 4: val_loss improved from 0.12666 to 0.11833, saving model to best_model.keras\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 132ms/step - auc: 0.9958 - loss: 0.0665 - macro_f1: 0.9097 - micro_f1: 0.9315 - subset_accuracy: 0.8803 - val_auc: 0.9711 - val_loss: 0.1183 - val_macro_f1: 0.8355 - val_micro_f1: 0.8582 - val_subset_accuracy: 0.7841\n","Epoch 5/50\n","\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9989 - loss: 0.0405 - macro_f1: 0.9576 - micro_f1: 0.9645 - subset_accuracy: 0.9337\n","Epoch 5: val_loss improved from 0.11833 to 0.11430, saving model to best_model.keras\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 132ms/step - auc: 0.9989 - loss: 0.0404 - macro_f1: 0.9577 - micro_f1: 0.9645 - subset_accuracy: 0.9338 - val_auc: 0.9713 - val_loss: 0.1143 - val_macro_f1: 0.8552 - val_micro_f1: 0.8683 - val_subset_accuracy: 0.7909\n","Epoch 6/50\n","\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9998 - loss: 0.0242 - macro_f1: 0.9820 - micro_f1: 0.9838 - subset_accuracy: 0.9684\n","Epoch 6: val_loss did not improve from 0.11430\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 123ms/step - auc: 0.9998 - loss: 0.0242 - macro_f1: 0.9820 - micro_f1: 0.9838 - subset_accuracy: 0.9685 - val_auc: 0.9669 - val_loss: 0.1199 - val_macro_f1: 0.8399 - val_micro_f1: 0.8558 - val_subset_accuracy: 0.7841\n","Epoch 7/50\n","\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9999 - loss: 0.0181 - macro_f1: 0.9871 - micro_f1: 0.9879 - subset_accuracy: 0.9771\n","Epoch 7: val_loss did not improve from 0.11430\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 123ms/step - auc: 0.9999 - loss: 0.0181 - macro_f1: 0.9871 - micro_f1: 0.9880 - subset_accuracy: 0.9772 - val_auc: 0.9664 - val_loss: 0.1233 - val_macro_f1: 0.8490 - val_micro_f1: 0.8654 - val_subset_accuracy: 0.7909\n","Epoch 8/50\n","\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9999 - loss: 0.0147 - macro_f1: 0.9877 - micro_f1: 0.9895 - subset_accuracy: 0.9797\n","Epoch 8: val_loss did not improve from 0.11430\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 123ms/step - auc: 0.9999 - loss: 0.0147 - macro_f1: 0.9877 - micro_f1: 0.9895 - subset_accuracy: 0.9797 - val_auc: 0.9637 - val_loss: 0.1281 - val_macro_f1: 0.8592 - val_micro_f1: 0.8719 - val_subset_accuracy: 0.8023\n","Epoch 9/50\n","\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 1.0000 - loss: 0.0099 - macro_f1: 0.9941 - micro_f1: 0.9947 - subset_accuracy: 0.9896\n","Epoch 9: val_loss did not improve from 0.11430\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 124ms/step - auc: 1.0000 - loss: 0.0099 - macro_f1: 0.9941 - micro_f1: 0.9946 - subset_accuracy: 0.9896 - val_auc: 0.9586 - val_loss: 0.1339 - val_macro_f1: 0.8528 - val_micro_f1: 0.8700 - val_subset_accuracy: 0.7977\n","Epoch 10/50\n","\u001b[1m144/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - auc: 0.9999 - loss: 0.0093 - macro_f1: 0.9928 - micro_f1: 0.9944 - subset_accuracy: 0.9901\n","Epoch 10: val_loss did not improve from 0.11430\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 123ms/step - auc: 0.9999 - loss: 0.0093 - macro_f1: 0.9928 - micro_f1: 0.9944 - subset_accuracy: 0.9901 - val_auc: 0.9694 - val_loss: 0.1239 - val_macro_f1: 0.8722 - val_micro_f1: 0.8853 - val_subset_accuracy: 0.8114\n","Epoch 10: early stopping\n","Restoring model weights from the end of the best epoch: 5.\n"]}]},{"cell_type":"markdown","source":["## Run predictions on test set"],"metadata":{"id":"83a7g8nff5n5"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import f1_score, accuracy_score\n","\n","# Load the test dataset\n","test_image_paths, test_labels, test_metadata = transform_coco_to_multilabel(\n","    dataset.location,\n","    subset='test',\n","    image_size=(640, 640)\n",")\n","\n","test_dataset = create_tf_dataset(\n","    test_image_paths,\n","    test_labels,\n","    test_metadata,\n",")\n","\n","# Load the best model (no custom objects needed)\n","best_model = tf.keras.models.load_model('best_model.keras', compile=False)\n","\n","# Generate predictions\n","print(\"Generating predictions...\")\n","y_pred_probs = best_model.predict(test_dataset, verbose=1)\n","y_pred = (y_pred_probs > 0.5).astype(int)\n","\n","# Get true labels\n","y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n","\n","# Calculate metrics\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Test Set Evaluation Metrics\")\n","print(\"=\" * 60)\n","\n","# Subset Accuracy (exact match ratio)\n","subset_accuracy = accuracy_score(y_true, y_pred)\n","print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n","\n","# Micro F1\n","micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n","print(f\"Micro F1:        {micro_f1:.4f}\")\n","\n","# Macro F1\n","macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n","print(f\"Macro F1:        {macro_f1:.4f}\")\n","\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SCi2cHdSxFxz","executionInfo":{"status":"ok","timestamp":1766912928022,"user_tz":360,"elapsed":36077,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"6dd1deb9-ab18-4d48-f5b9-3e6c1575f2b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded test set: 218 images, 7 classes\n","Labels shape: (218, 7)\n","Generating predictions...\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 4s/step\n","\n","============================================================\n","Test Set Evaluation Metrics\n","============================================================\n","Subset Accuracy: 0.8303\n","Micro F1:        0.8846\n","Macro F1:        0.8724\n","============================================================\n"]}]}]}