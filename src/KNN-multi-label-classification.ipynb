{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvlC2Rnni7oi"
   },
   "source": [
    "# Junk Food Multi-label Classification with KNN\n",
    "\n",
    "This notebook implements a **K-Nearest Neighbors (KNN)** model for image classification from a **COCO JSON dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcbOFYLzjV6B"
   },
   "source": [
    "## Before you start\n",
    "\n",
    "Make sure you have access to GPU. In case of any problems, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, click `Save` and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1766911489333,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "bjlhPWyOjWsp",
    "outputId": "3aaff2b9-ba5e-4ef8-bc9b-b9d9d8216171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 28 08:44:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   70C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766911489341,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "5H3H-YHDkdqz",
    "outputId": "46a24cb7-9a0d-4fa6-e8b1-a2ebb6cc16a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME: /content\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1766911489387,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "xe-7ZMI6kebP",
    "outputId": "d40719fe-34a3-4a38-dd43-917a5ae5ca10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datasets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p {HOME}/datasets\n",
    "%cd {HOME}/datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH1xxtULn2xV"
   },
   "source": [
    "## Install packages using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 22348,
     "status": "ok",
     "timestamp": 1766911511738,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "xvTEqLaOkjPL",
    "outputId": "7d1a21c1-7ee9-41bb-aaea-ba98fe153292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting roboflow==1.2.11\n",
      "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision==0.24.0 in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2025.11.12)\n",
      "Collecting idna==3.7 (from roboflow==1.2.11)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.4.9)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.0.2)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow==1.2.11)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (11.3.0)\n",
      "Collecting pi-heif<2 (from roboflow==1.2.11)\n",
      "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pillow-avif-plugin<2 (from roboflow==1.2.11)\n",
      "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.32.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.5.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (6.0.3)\n",
      "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.0.0)\n",
      "Collecting filetype (from roboflow==1.2.11)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (1.3.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (4.61.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (3.2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow==1.2.11) (3.4.4)\n",
      "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.12.0.88\n",
      "    Uninstalling opencv-python-headless-4.12.0.88:\n",
      "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.11\n",
      "    Uninstalling idna-3.11:\n",
      "      Successfully uninstalled idna-3.11\n",
      "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow==1.2.11 torch==2.9.0 torchvision==0.24.0 scikit-learn==1.6.1 tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CCGi6EWnsMN"
   },
   "source": [
    "## Download dataset from Roboflow\n",
    "\n",
    "Don't forget to change the `API_KEY` with your dataset key.\n",
    "\n",
    "We replicate your original dataset setup. Even though the dataset is labeled for object detection, we’ll use the full image classification approach with KNN. Labels will be derived from the most frequent class per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16993,
     "status": "ok",
     "timestamp": 1766911528738,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "XM4b5M09kj3D",
    "outputId": "71a804bb-e588-4e8a-c9a9-1ff43cf8e7a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Junk-Food-Detection-10 to coco:: 100%|██████████| 293482/293482 [00:10<00:00, 27415.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Junk-Food-Detection-10 in coco:: 100%|██████████| 5280/5280 [00:02<00:00, 2024.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "from google.colab import userdata\n",
    "\n",
    "rf = Roboflow(api_key=userdata.get('ROBOFLOW_API_KEY'))\n",
    "project = rf.workspace(userdata.get('ROBOFLOW_WORKSPACE_ID')).project(userdata.get('ROBOFLOW_PROJECT_ID'))\n",
    "version = project.version(userdata.get('ROBOFLOW_DATASET_VERSION'))\n",
    "dataset = version.download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1766911528852,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "71fxf9Tsktei",
    "outputId": "3376f409-b449-49ec-f2dc-09d0445ee1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM3porFAovJj"
   },
   "source": [
    "## Dataset Loading and Label Extraction\n",
    "\n",
    "We will use a classification model. So, for labeling, we use two classes: junk-food-ad and non-junk-food-ad. Given the fact that the dataset is multiclass, the rule is: if there is at least one bounding box belonging to a particular image, it's junk-food-ad. Otherwise, it's non-junk-food-ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1766911528904,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "_QaNLLGlie3V",
    "outputId": "64be2aa4-1d1e-49cc-f0e0-11ae446620d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATASET SUMMARY (Multi-label)\n",
      "==================================================\n",
      "\n",
      "Classes (7): ['french_fries', 'fried_chicken', 'hamburger', 'ice_cream', 'junk_food_logo', 'pizza', 'soda']\n",
      "\n",
      "Dataset parts processed:\n",
      "\n",
      "TRAIN:\n",
      "  Total images: 4614\n",
      "  Label matrix shape: (4614, 7)\n",
      "  Label distribution:\n",
      "    - french_fries: 388 (8.4%)\n",
      "    - fried_chicken: 315 (6.8%)\n",
      "    - hamburger: 379 (8.2%)\n",
      "    - ice_cream: 468 (10.1%)\n",
      "    - junk_food_logo: 1863 (40.4%)\n",
      "    - pizza: 411 (8.9%)\n",
      "    - soda: 603 (13.1%)\n",
      "  Labels per image:\n",
      "    - Mean: 0.96\n",
      "    - Min: 0\n",
      "    - Max: 5\n",
      "    - Images with 0 labels: 1689\n",
      "\n",
      "VALID:\n",
      "  Total images: 440\n",
      "  Label matrix shape: (440, 7)\n",
      "  Label distribution:\n",
      "    - french_fries: 41 (9.3%)\n",
      "    - fried_chicken: 36 (8.2%)\n",
      "    - hamburger: 34 (7.7%)\n",
      "    - ice_cream: 42 (9.5%)\n",
      "    - junk_food_logo: 180 (40.9%)\n",
      "    - pizza: 40 (9.1%)\n",
      "    - soda: 67 (15.2%)\n",
      "  Labels per image:\n",
      "    - Mean: 1.00\n",
      "    - Min: 0\n",
      "    - Max: 4\n",
      "    - Images with 0 labels: 163\n",
      "\n",
      "TEST:\n",
      "  Total images: 218\n",
      "  Label matrix shape: (218, 7)\n",
      "  Label distribution:\n",
      "    - french_fries: 18 (8.3%)\n",
      "    - fried_chicken: 22 (10.1%)\n",
      "    - hamburger: 15 (6.9%)\n",
      "    - ice_cream: 19 (8.7%)\n",
      "    - junk_food_logo: 82 (37.6%)\n",
      "    - pizza: 15 (6.9%)\n",
      "    - soda: 20 (9.2%)\n",
      "  Labels per image:\n",
      "    - Mean: 0.88\n",
      "    - Min: 0\n",
      "    - Max: 5\n",
      "    - Images with 0 labels: 87\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_coco_annotations(json_path: str) -> Dict:\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def process_dataset_part(\n",
    "    part_dir: str,\n",
    "    annotations_filename: str = \"_annotations.coco.json\"\n",
    ") -> Tuple[List[str], np.ndarray, List[str]]:\n",
    "    annotations_path = os.path.join(part_dir, annotations_filename)\n",
    "\n",
    "    if not os.path.exists(annotations_path):\n",
    "        raise FileNotFoundError(f\"Annotations file not found: {annotations_path}\")\n",
    "\n",
    "    # Load annotations\n",
    "    coco_data = load_coco_annotations(annotations_path)\n",
    "\n",
    "    # Create category mapping, excluding \"junk-food\"\n",
    "    category_id_to_name = {\n",
    "        cat['id']: cat['name']\n",
    "        for cat in coco_data['categories']\n",
    "        if cat['name'] != 'junk-food'\n",
    "    }\n",
    "    all_category_names = sorted(set(category_id_to_name.values()))\n",
    "\n",
    "    # Create a mapping of image_id to set of category names\n",
    "    image_to_categories = {}\n",
    "    for annotation in coco_data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        category_id = annotation['category_id']\n",
    "\n",
    "        # Skip if category is not in our filtered mapping\n",
    "        if category_id not in category_id_to_name:\n",
    "            continue\n",
    "\n",
    "        category_name = category_id_to_name[category_id]\n",
    "\n",
    "        if image_id not in image_to_categories:\n",
    "            image_to_categories[image_id] = set()\n",
    "        image_to_categories[image_id].add(category_name)\n",
    "\n",
    "    # Process images in order\n",
    "    image_paths = []\n",
    "    labels_list = []\n",
    "\n",
    "    for image_info in coco_data['images']:\n",
    "        image_id = image_info['id']\n",
    "        file_name = image_info['file_name']\n",
    "\n",
    "        image_path = os.path.join(part_dir, file_name)\n",
    "        image_paths.append(image_path)\n",
    "\n",
    "        # Create multi-hot encoded label vector\n",
    "        label_vector = np.zeros(len(all_category_names), dtype=int)\n",
    "        if image_id in image_to_categories:\n",
    "            for category_name in image_to_categories[image_id]:\n",
    "                idx = all_category_names.index(category_name)\n",
    "                label_vector[idx] = 1\n",
    "\n",
    "        labels_list.append(label_vector)\n",
    "\n",
    "    labels_array = np.array(labels_list)\n",
    "    return image_paths, labels_array, all_category_names\n",
    "\n",
    "\n",
    "def process_full_dataset(\n",
    "    dataset_root: str,\n",
    "    parts: List[str] = ['train', 'valid', 'test']\n",
    ") -> Tuple[Dict[str, List[str]], Dict[str, np.ndarray], List[str]]:\n",
    "\n",
    "    all_image_paths = {}\n",
    "    all_labels = {}\n",
    "    classes = None\n",
    "\n",
    "    for part in parts:\n",
    "        part_dir = os.path.join(dataset_root, part)\n",
    "\n",
    "        if not os.path.exists(part_dir):\n",
    "            print(f\"Warning: Directory not found: {part_dir}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        image_paths, labels, part_classes = process_dataset_part(part_dir)\n",
    "\n",
    "        # Ensure all parts have the same classes\n",
    "        if classes is None:\n",
    "            classes = part_classes\n",
    "        elif classes != part_classes:\n",
    "            print(f\"Warning: Classes differ in {part}. Using classes from first part.\")\n",
    "\n",
    "        all_image_paths[part] = image_paths\n",
    "        all_labels[part] = labels\n",
    "\n",
    "    return all_image_paths, all_labels, classes\n",
    "\n",
    "\n",
    "image_paths_dict, labels_dict, classes = process_full_dataset(dataset.location)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET SUMMARY (Multi-label)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nClasses ({len(classes)}): {classes}\")\n",
    "print(f\"\\nDataset parts processed:\")\n",
    "\n",
    "for part in image_paths_dict.keys():\n",
    "    print(f\"\\n{part.upper()}:\")\n",
    "    print(f\"  Total images: {len(image_paths_dict[part])}\")\n",
    "    print(f\"  Label matrix shape: {labels_dict[part].shape}\")\n",
    "    print(f\"  Label distribution:\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        count = labels_dict[part][:, i].sum()\n",
    "        percentage = (count / len(labels_dict[part]) * 100) if len(labels_dict[part]) > 0 else 0\n",
    "        print(f\"    - {cls}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Multi-label statistics\n",
    "    labels_per_image = labels_dict[part].sum(axis=1)\n",
    "    print(f\"  Labels per image:\")\n",
    "    print(f\"    - Mean: {labels_per_image.mean():.2f}\")\n",
    "    print(f\"    - Min: {labels_per_image.min()}\")\n",
    "    print(f\"    - Max: {labels_per_image.max()}\")\n",
    "    print(f\"    - Images with 0 labels: {(labels_per_image == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJqUcsBxo6X-"
   },
   "source": [
    "## Feature Extraction of train set using pretrained models\n",
    "\n",
    "KNN itself cannot extract visual features, it only compares numeric vectors.  \n",
    "Therefore, we use **pre-trained** models (without their classification heads) to extract image embeddings of train set.\n",
    "\n",
    "These embeddings (feature vectors) represent each image in a high-dimensional space that captures visual similarity.  \n",
    "The extracted features are stored as a NumPy matrix and later fed into the KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 854012,
     "status": "ok",
     "timestamp": 1766912382918,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "EsJu3KLAk4ji",
    "outputId": "aed469c7-4900-4f29-86e3-441a32cad5e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Downloading: \"https://download.pytorch.org/models/resnext101_32x8d-110c445d.pth\" to /root/.cache/torch/hub/checkpoints/resnext101_32x8d-110c445d.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340M/340M [00:06<00:00, 53.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_m-dc08266a.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_m-dc08266a.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208M/208M [00:01<00:00, 204MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/convnext_base-6075fbad.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base-6075fbad.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338M/338M [00:02<00:00, 133MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330M/330M [00:01<00:00, 201MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_v2_b-781e5279.pth\" to /root/.cache/torch/hub/checkpoints/swin_v2_b-781e5279.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336M/336M [00:01<00:00, 185MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers is not available (SwiGLU)\n",
      "xFormers is not available (Attention)\n",
      "xFormers is not available (Block)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitb14_pretrain.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330M/330M [00:00<00:00, 428MB/s]\n",
      "Extracting features - ResNeXt-101: 100%|██████████| 4614/4614 [02:05<00:00, 36.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape - ResNeXt-101: (4614, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features - EfficientNet V2: 100%|██████████| 4614/4614 [03:00<00:00, 25.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape - EfficientNet V2: (4614, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features - ConvNeXt: 100%|██████████| 4614/4614 [01:31<00:00, 50.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape - ConvNeXt: (4614, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features - ViT: 100%|██████████| 4614/4614 [01:25<00:00, 53.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape - ViT: (4614, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features - Swin Transformer: 100%|██████████| 4614/4614 [03:21<00:00, 22.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape - Swin Transformer: (4614, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features - DINOv2: 100%|██████████| 4614/4614 [02:02<00:00, 37.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape - DINOv2: (4614, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(image_paths, model, feature_dim, transform, model_name):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for path in tqdm(image_paths, desc=f\"Extracting features - {model_name}\"):\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                tensor = transform(img).unsqueeze(0).to(device)\n",
    "                feat = model(tensor).squeeze().cpu().numpy()\n",
    "                features.append(feat)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {path}: {e}\")\n",
    "                features.append(np.zeros(feature_dim))\n",
    "    return np.array(features)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_configs = [\n",
    "    {\n",
    "        'name': 'ResNeXt-101',\n",
    "        'loader': lambda: models.resnext101_32x8d(weights=models.ResNeXt101_32X8D_Weights.DEFAULT),\n",
    "        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n",
    "        'transform': models.ResNet50_Weights.DEFAULT.transforms(),\n",
    "        'feature_dim': 2048\n",
    "    },\n",
    "    {\n",
    "        'name': 'EfficientNet V2',\n",
    "        'loader': lambda: models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.DEFAULT),\n",
    "        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n",
    "        'transform': models.EfficientNet_V2_M_Weights.DEFAULT.transforms(),\n",
    "        'feature_dim': 1280\n",
    "    },\n",
    "    {\n",
    "        'name': 'ConvNeXt',\n",
    "        'loader': lambda: models.convnext_base(weights=models.ConvNeXt_Base_Weights.DEFAULT),\n",
    "        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n",
    "        'transform': models.ConvNeXt_Base_Weights.DEFAULT.transforms(),\n",
    "        'feature_dim': 1024\n",
    "    },\n",
    "    {\n",
    "        'name': 'ViT',\n",
    "        'loader': lambda: models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT),\n",
    "        'modifier': lambda m: (setattr(m.heads, 'head', torch.nn.Identity()), m)[1],\n",
    "        'transform': models.ViT_B_16_Weights.DEFAULT.transforms(),\n",
    "        'feature_dim': 768\n",
    "    },\n",
    "    {\n",
    "        'name': 'Swin Transformer',\n",
    "        'loader': lambda: models.swin_v2_b(weights=models.Swin_V2_B_Weights.DEFAULT),\n",
    "        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n",
    "        'transform': models.Swin_B_Weights.DEFAULT.transforms(),\n",
    "        'feature_dim': 1024\n",
    "    },\n",
    "    {\n",
    "        'name': 'DINOv2',\n",
    "        'loader': lambda: torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14'),\n",
    "        'modifier': lambda m: m,  # No modification needed\n",
    "        'transform': transforms.Compose([\n",
    "            transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        'feature_dim': 768\n",
    "    }\n",
    "]\n",
    "\n",
    "loaded_models = []\n",
    "for config in model_configs:\n",
    "    model = config['loader']()\n",
    "    model = config['modifier'](model)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    loaded_models.append({\n",
    "        'model': model,\n",
    "        'name': config['name'],\n",
    "        'transform': config['transform'],\n",
    "        'feature_dim': config['feature_dim']\n",
    "    })\n",
    "\n",
    "# Extract features for all models on train set\n",
    "all_features = {}\n",
    "for model_info in loaded_models:\n",
    "    features = extract_features(\n",
    "        image_paths_dict['train'],\n",
    "        model_info['model'],\n",
    "        model_info['feature_dim'],\n",
    "        model_info['transform'],\n",
    "        model_info['name']\n",
    "    )\n",
    "    all_features[model_info['name']] = features\n",
    "    print(f\"Feature matrix shape - {model_info['name']}: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jJde8topGt6"
   },
   "source": [
    "## \"Training\" the KNN Classifiers\n",
    "\n",
    "KNN is trained (fitted) using a simple distance-based rule:\n",
    "- Each image is classified based on the majority vote of its *k* nearest neighbors in the feature space.\n",
    "- We use `k=5` neighbors for this experiment.\n",
    "\n",
    "After training, we compute accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6399,
     "status": "ok",
     "timestamp": 1766912389333,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "3ksSk59glBFq",
    "outputId": "8c4d88ba-2230-48d3-c932-bdec45672272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Multi-label KNN for ResNeXt-101\n",
      "Trained Multi-label KNN for EfficientNet V2\n",
      "Trained Multi-label KNN for ConvNeXt\n",
      "Trained Multi-label KNN for ViT\n",
      "Trained Multi-label KNN for Swin Transformer\n",
      "Trained Multi-label KNN for DINOv2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "RESULTS_PATH = os.path.join(HOME, \"runs\", \"classify\")\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "y_train = labels_dict['train']\n",
    "\n",
    "# Train KNN classifiers with MultiOutputClassifier for all models\n",
    "trained_models = {}\n",
    "for model_name, features in all_features.items():\n",
    "    base_knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "    multi_label_knn = MultiOutputClassifier(base_knn, n_jobs=-1)\n",
    "    multi_label_knn.fit(features, y_train)\n",
    "\n",
    "    trained_models[model_name] = multi_label_knn\n",
    "    print(f\"Trained Multi-label KNN for {model_name}\")\n",
    "\n",
    "\n",
    "def evaluate_model(X, y, split_name, model, model_name):\n",
    "    split_dir = os.path.join(RESULTS_PATH, split_name)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Subset Accuracy (exact match ratio)\n",
    "    subset_accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "    # Micro F1 (aggregate across all label-sample pairs)\n",
    "    micro_f1 = f1_score(y, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "    # Macro F1 (average F1 across labels)\n",
    "    macro_f1 = f1_score(y, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'subset_accuracy': subset_accuracy,\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'y_pred': y_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTkeOOmipW6i"
   },
   "source": [
    "## Predictions on valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81814,
     "status": "ok",
     "timestamp": 1766912471152,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "4Gqv0MJIlDSC",
    "outputId": "82a79434-fc28-405a-da15-35dd0353eac9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features - ResNeXt-101: 100%|██████████| 440/440 [00:11<00:00, 37.36it/s]\n",
      "Extracting features - EfficientNet V2: 100%|██████████| 440/440 [00:17<00:00, 25.45it/s]\n",
      "Extracting features - ConvNeXt: 100%|██████████| 440/440 [00:08<00:00, 54.47it/s]\n",
      "Extracting features - ViT: 100%|██████████| 440/440 [00:08<00:00, 53.98it/s]\n",
      "Extracting features - Swin Transformer: 100%|██████████| 440/440 [00:19<00:00, 22.86it/s]\n",
      "Extracting features - DINOv2: 100%|██████████| 440/440 [00:11<00:00, 38.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXt-101 - Valid Set:\n",
      "  Subset Accuracy: 0.7409\n",
      "  Micro F1:        0.8153\n",
      "  Macro F1:        0.7816\n",
      "\n",
      "EfficientNet V2 - Valid Set:\n",
      "  Subset Accuracy: 0.7455\n",
      "  Micro F1:        0.8142\n",
      "  Macro F1:        0.7957\n",
      "\n",
      "ConvNeXt - Valid Set:\n",
      "  Subset Accuracy: 0.7455\n",
      "  Micro F1:        0.8089\n",
      "  Macro F1:        0.7850\n",
      "\n",
      "ViT - Valid Set:\n",
      "  Subset Accuracy: 0.7591\n",
      "  Micro F1:        0.8289\n",
      "  Macro F1:        0.8083\n",
      "\n",
      "Swin Transformer - Valid Set:\n",
      "  Subset Accuracy: 0.7500\n",
      "  Micro F1:        0.8310\n",
      "  Macro F1:        0.8187\n",
      "\n",
      "DINOv2 - Valid Set:\n",
      "  Subset Accuracy: 0.7886\n",
      "  Micro F1:        0.8530\n",
      "  Macro F1:        0.8581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract features for validation set\n",
    "all_valid_features = {}\n",
    "for model_info in loaded_models:\n",
    "    features = extract_features(\n",
    "        image_paths_dict['valid'],\n",
    "        model_info['model'],\n",
    "        model_info['feature_dim'],\n",
    "        model_info['transform'],\n",
    "        model_info['name']\n",
    "    )\n",
    "    all_valid_features[model_info['name']] = features\n",
    "\n",
    "y_valid = labels_dict['valid']\n",
    "\n",
    "# Evaluate all models on valid set\n",
    "for model_name, model in trained_models.items():\n",
    "    results = evaluate_model(all_valid_features[model_name], y_valid, \"valid\", model, model_name)\n",
    "    print(f\"{model_name} - Valid Set:\")\n",
    "    print(f\"  Subset Accuracy: {results['subset_accuracy']:.4f}\")\n",
    "    print(f\"  Micro F1:        {results['micro_f1']:.4f}\")\n",
    "    print(f\"  Macro F1:        {results['macro_f1']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRwWXMjO8yBe"
   },
   "source": [
    "## Metrics on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41533,
     "status": "ok",
     "timestamp": 1766912512698,
     "user": {
      "displayName": "Jose Andrés",
      "userId": "08637818255854367082"
     },
     "user_tz": 360
    },
    "id": "_juqzFfV7pVH",
    "outputId": "bf81aaea-71de-412a-db36-caa1fa5c56a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features - ResNeXt-101: 100%|██████████| 218/218 [00:06<00:00, 35.64it/s]\n",
      "Extracting features - EfficientNet V2: 100%|██████████| 218/218 [00:07<00:00, 27.76it/s]\n",
      "Extracting features - ConvNeXt: 100%|██████████| 218/218 [00:04<00:00, 47.89it/s]\n",
      "Extracting features - ViT: 100%|██████████| 218/218 [00:03<00:00, 54.97it/s]\n",
      "Extracting features - Swin Transformer: 100%|██████████| 218/218 [00:09<00:00, 22.88it/s]\n",
      "Extracting features - DINOv2: 100%|██████████| 218/218 [00:05<00:00, 39.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXt-101 - Test Set:\n",
      "  Subset Accuracy: 0.7202\n",
      "  Micro F1:        0.7928\n",
      "  Macro F1:        0.8051\n",
      "\n",
      "EfficientNet V2 - Test Set:\n",
      "  Subset Accuracy: 0.7156\n",
      "  Micro F1:        0.7959\n",
      "  Macro F1:        0.8047\n",
      "\n",
      "ConvNeXt - Test Set:\n",
      "  Subset Accuracy: 0.7385\n",
      "  Micro F1:        0.8117\n",
      "  Macro F1:        0.8319\n",
      "\n",
      "ViT - Test Set:\n",
      "  Subset Accuracy: 0.7706\n",
      "  Micro F1:        0.8293\n",
      "  Macro F1:        0.8343\n",
      "\n",
      "Swin Transformer - Test Set:\n",
      "  Subset Accuracy: 0.7294\n",
      "  Micro F1:        0.8128\n",
      "  Macro F1:        0.8178\n",
      "\n",
      "DINOv2 - Test Set:\n",
      "  Subset Accuracy: 0.7936\n",
      "  Micro F1:        0.8722\n",
      "  Macro F1:        0.8915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_test_features = {}\n",
    "for model_info in loaded_models:\n",
    "    features = extract_features(\n",
    "        image_paths_dict['test'],\n",
    "        model_info['model'],\n",
    "        model_info['feature_dim'],\n",
    "        model_info['transform'],\n",
    "        model_info['name']\n",
    "    )\n",
    "    all_test_features[model_info['name']] = features\n",
    "\n",
    "y_test = labels_dict['test']\n",
    "\n",
    "# Evaluate all models on test set\n",
    "for model_name, model in trained_models.items():\n",
    "    results = evaluate_model(all_test_features[model_name], y_test, \"test\", model, model_name)\n",
    "    print(f\"{model_name} - Test Set:\")\n",
    "    print(f\"  Subset Accuracy: {results['subset_accuracy']:.4f}\")\n",
    "    print(f\"  Micro F1:        {results['micro_f1']:.4f}\")\n",
    "    print(f\"  Macro F1:        {results['macro_f1']:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
