{"cells":[{"cell_type":"markdown","metadata":{"id":"EvlC2Rnni7oi"},"source":["# Junk Food Multi-label Classification with KNN\n","\n","This notebook implements a **K-Nearest Neighbors (KNN)** model for image classification from a **COCO JSON dataset**."]},{"cell_type":"markdown","metadata":{"id":"gcbOFYLzjV6B"},"source":["## Before you start\n","\n","Make sure you have access to GPU. In case of any problems, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, click `Save` and try again."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjlhPWyOjWsp","outputId":"b4fb7f79-bbd2-4517-d042-106a0be37e87","executionInfo":{"status":"ok","timestamp":1768725645125,"user_tz":360,"elapsed":153,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Jan 18 08:40:54 2026       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   58C    P0             28W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5H3H-YHDkdqz","outputId":"8aab3174-f5bf-4c03-c453-be1c0148b6f1","executionInfo":{"status":"ok","timestamp":1768725645130,"user_tz":360,"elapsed":4,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["HOME: /content\n"]}],"source":["import os\n","HOME = os.getcwd()\n","print(\"HOME:\", HOME)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xe-7ZMI6kebP","outputId":"b8e0f24e-eb59-4f75-d1c1-3e65667f70ba","executionInfo":{"status":"ok","timestamp":1768725645195,"user_tz":360,"elapsed":63,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/datasets\n"]}],"source":["!mkdir -p {HOME}/datasets\n","%cd {HOME}/datasets\n"]},{"cell_type":"markdown","metadata":{"id":"mH1xxtULn2xV"},"source":["## Install packages using pip"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"xvTEqLaOkjPL","outputId":"88cc04c4-b8c6-4de1-8acc-a0ff23368525","executionInfo":{"status":"ok","timestamp":1768725657859,"user_tz":360,"elapsed":12658,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: roboflow==1.2.11 in /usr/local/lib/python3.12/dist-packages (1.2.11)\n","Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: torchvision==0.24.0 in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2026.1.4)\n","Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (3.7)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.4.9)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (3.10.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.0.2)\n","Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (4.10.0.84)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (11.3.0)\n","Requirement already satisfied: pi-heif<2 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.1.1)\n","Requirement already satisfied: pillow-avif-plugin<2 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.5.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.9.0.post0)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.2.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.32.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.17.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.5.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (6.0.3)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.0.0)\n","Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.20.2)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.5.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (1.5.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1) (3.6.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0) (3.0.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (1.3.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (4.61.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (3.3.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow==1.2.11) (3.4.4)\n"]}],"source":["!pip install roboflow==1.2.11 torch==2.9.0 torchvision==0.24.0 scikit-learn==1.6.1 tqdm==4.67.1"]},{"cell_type":"markdown","metadata":{"id":"_CCGi6EWnsMN"},"source":["## Download dataset from Roboflow\n","\n","Don't forget to change the `API_KEY` with your dataset key.\n","\n","We replicate your original dataset setup. Even though the dataset is labeled for object detection, we’ll use the full image classification approach with KNN. Labels will be derived from the most frequent class per image."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XM4b5M09kj3D","outputId":"f1d565a9-3656-47d6-9dcf-3fdd94c193d9","executionInfo":{"status":"ok","timestamp":1768725660404,"user_tz":360,"elapsed":2537,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]}],"source":["from roboflow import Roboflow\n","from google.colab import userdata\n","\n","rf = Roboflow(api_key=userdata.get('ROBOFLOW_API_KEY'))\n","project = rf.workspace(userdata.get('ROBOFLOW_WORKSPACE_ID')).project(userdata.get('ROBOFLOW_PROJECT_ID'))\n","version = project.version(userdata.get('ROBOFLOW_DATASET_VERSION'))\n","dataset = version.download(\"coco\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71fxf9Tsktei","outputId":"e077c6e1-5e36-485d-81a8-c5b4f44c0244","executionInfo":{"status":"ok","timestamp":1768725660413,"user_tz":360,"elapsed":6,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["%cd {HOME}"]},{"cell_type":"markdown","metadata":{"id":"LM3porFAovJj"},"source":["## Dataset Loading and Label Extraction\n","\n","We will use a classification model. So, for labeling, we use two classes: junk-food-ad and non-junk-food-ad. Given the fact that the dataset is multiclass, the rule is: if there is at least one bounding box belonging to a particular image, it's junk-food-ad. Otherwise, it's non-junk-food-ad"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_QaNLLGlie3V","outputId":"30e90d13-9955-45e4-e9ba-800fc89996d1","executionInfo":{"status":"ok","timestamp":1768725660591,"user_tz":360,"elapsed":173,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","DATASET SUMMARY (Multi-label)\n","==================================================\n","\n","Classes (7): ['french_fries', 'fried_chicken', 'hamburger', 'ice_cream', 'junk_food_logo', 'pizza', 'soda']\n","\n","Dataset parts processed:\n","\n","TRAIN:\n","  Total images: 4614\n","  Label matrix shape: (4614, 7)\n","  Label distribution:\n","    - french_fries: 388 (8.4%)\n","    - fried_chicken: 315 (6.8%)\n","    - hamburger: 379 (8.2%)\n","    - ice_cream: 468 (10.1%)\n","    - junk_food_logo: 1863 (40.4%)\n","    - pizza: 411 (8.9%)\n","    - soda: 603 (13.1%)\n","  Labels per image:\n","    - Mean: 0.96\n","    - Min: 0\n","    - Max: 5\n","    - Images with 0 labels: 1689\n","\n","VALID:\n","  Total images: 440\n","  Label matrix shape: (440, 7)\n","  Label distribution:\n","    - french_fries: 41 (9.3%)\n","    - fried_chicken: 36 (8.2%)\n","    - hamburger: 34 (7.7%)\n","    - ice_cream: 42 (9.5%)\n","    - junk_food_logo: 180 (40.9%)\n","    - pizza: 40 (9.1%)\n","    - soda: 67 (15.2%)\n","  Labels per image:\n","    - Mean: 1.00\n","    - Min: 0\n","    - Max: 4\n","    - Images with 0 labels: 163\n","\n","TEST:\n","  Total images: 218\n","  Label matrix shape: (218, 7)\n","  Label distribution:\n","    - french_fries: 18 (8.3%)\n","    - fried_chicken: 22 (10.1%)\n","    - hamburger: 15 (6.9%)\n","    - ice_cream: 19 (8.7%)\n","    - junk_food_logo: 82 (37.6%)\n","    - pizza: 15 (6.9%)\n","    - soda: 20 (9.2%)\n","  Labels per image:\n","    - Mean: 0.88\n","    - Min: 0\n","    - Max: 5\n","    - Images with 0 labels: 87\n"]}],"source":["import json\n","import os\n","from pathlib import Path\n","from typing import Dict, List, Tuple\n","import numpy as np\n","\n","\n","def load_coco_annotations(json_path: str) -> Dict:\n","    with open(json_path, 'r') as f:\n","        return json.load(f)\n","\n","\n","def process_dataset_part(\n","    part_dir: str,\n","    annotations_filename: str = \"_annotations.coco.json\"\n",") -> Tuple[List[str], np.ndarray, List[str]]:\n","    annotations_path = os.path.join(part_dir, annotations_filename)\n","\n","    if not os.path.exists(annotations_path):\n","        raise FileNotFoundError(f\"Annotations file not found: {annotations_path}\")\n","\n","    # Load annotations\n","    coco_data = load_coco_annotations(annotations_path)\n","\n","    # Create category mapping, excluding \"junk-food\"\n","    category_id_to_name = {\n","        cat['id']: cat['name']\n","        for cat in coco_data['categories']\n","        if cat['name'] != 'junk-food'\n","    }\n","    all_category_names = sorted(set(category_id_to_name.values()))\n","\n","    # Create a mapping of image_id to set of category names\n","    image_to_categories = {}\n","    for annotation in coco_data['annotations']:\n","        image_id = annotation['image_id']\n","        category_id = annotation['category_id']\n","\n","        # Skip if category is not in our filtered mapping\n","        if category_id not in category_id_to_name:\n","            continue\n","\n","        category_name = category_id_to_name[category_id]\n","\n","        if image_id not in image_to_categories:\n","            image_to_categories[image_id] = set()\n","        image_to_categories[image_id].add(category_name)\n","\n","    # Process images in order\n","    image_paths = []\n","    labels_list = []\n","\n","    for image_info in coco_data['images']:\n","        image_id = image_info['id']\n","        file_name = image_info['file_name']\n","\n","        image_path = os.path.join(part_dir, file_name)\n","        image_paths.append(image_path)\n","\n","        # Create multi-hot encoded label vector\n","        label_vector = np.zeros(len(all_category_names), dtype=int)\n","        if image_id in image_to_categories:\n","            for category_name in image_to_categories[image_id]:\n","                idx = all_category_names.index(category_name)\n","                label_vector[idx] = 1\n","\n","        labels_list.append(label_vector)\n","\n","    labels_array = np.array(labels_list)\n","    return image_paths, labels_array, all_category_names\n","\n","\n","def process_full_dataset(\n","    dataset_root: str,\n","    parts: List[str] = ['train', 'valid', 'test']\n",") -> Tuple[Dict[str, List[str]], Dict[str, np.ndarray], List[str]]:\n","\n","    all_image_paths = {}\n","    all_labels = {}\n","    classes = None\n","\n","    for part in parts:\n","        part_dir = os.path.join(dataset_root, part)\n","\n","        if not os.path.exists(part_dir):\n","            print(f\"Warning: Directory not found: {part_dir}. Skipping...\")\n","            continue\n","\n","        image_paths, labels, part_classes = process_dataset_part(part_dir)\n","\n","        # Ensure all parts have the same classes\n","        if classes is None:\n","            classes = part_classes\n","        elif classes != part_classes:\n","            print(f\"Warning: Classes differ in {part}. Using classes from first part.\")\n","\n","        all_image_paths[part] = image_paths\n","        all_labels[part] = labels\n","\n","    return all_image_paths, all_labels, classes\n","\n","\n","image_paths_dict, labels_dict, classes = process_full_dataset(dataset.location)\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"DATASET SUMMARY (Multi-label)\")\n","print(\"=\"*50)\n","print(f\"\\nClasses ({len(classes)}): {classes}\")\n","print(f\"\\nDataset parts processed:\")\n","\n","for part in image_paths_dict.keys():\n","    print(f\"\\n{part.upper()}:\")\n","    print(f\"  Total images: {len(image_paths_dict[part])}\")\n","    print(f\"  Label matrix shape: {labels_dict[part].shape}\")\n","    print(f\"  Label distribution:\")\n","    for i, cls in enumerate(classes):\n","        count = labels_dict[part][:, i].sum()\n","        percentage = (count / len(labels_dict[part]) * 100) if len(labels_dict[part]) > 0 else 0\n","        print(f\"    - {cls}: {count} ({percentage:.1f}%)\")\n","\n","    # Multi-label statistics\n","    labels_per_image = labels_dict[part].sum(axis=1)\n","    print(f\"  Labels per image:\")\n","    print(f\"    - Mean: {labels_per_image.mean():.2f}\")\n","    print(f\"    - Min: {labels_per_image.min()}\")\n","    print(f\"    - Max: {labels_per_image.max()}\")\n","    print(f\"    - Images with 0 labels: {(labels_per_image == 0).sum()}\")"]},{"cell_type":"markdown","metadata":{"id":"ZJqUcsBxo6X-"},"source":["## Feature Extraction of train set using pretrained models\n","\n","KNN itself cannot extract visual features, it only compares numeric vectors.  \n","Therefore, we use **pre-trained** models (without their classification heads) to extract image embeddings of train set.\n","\n","These embeddings (feature vectors) represent each image in a high-dimensional space that captures visual similarity.  \n","The extracted features are stored as a NumPy matrix and later fed into the KNN classifier."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsJu3KLAk4ji","outputId":"8885c866-76a0-4be8-e0bf-1afe5dec6f23","executionInfo":{"status":"ok","timestamp":1768726527646,"user_tz":360,"elapsed":867047,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n","xFormers is not available (SwiGLU)\n","xFormers is not available (Attention)\n","xFormers is not available (Block)\n","Extracting features - ResNeXt-101: 100%|██████████| 4614/4614 [02:13<00:00, 34.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - ResNeXt-101: (4614, 2048)\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features - EfficientNet V2: 100%|██████████| 4614/4614 [03:09<00:00, 24.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - EfficientNet V2: (4614, 1280)\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features - ConvNeXt: 100%|██████████| 4614/4614 [01:43<00:00, 44.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - ConvNeXt: (4614, 1024)\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features - ViT: 100%|██████████| 4614/4614 [01:27<00:00, 53.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - ViT: (4614, 768)\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features - Swin Transformer: 100%|██████████| 4614/4614 [03:22<00:00, 22.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - Swin Transformer: (4614, 1024)\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features - DINOv2: 100%|██████████| 4614/4614 [02:03<00:00, 37.23it/s]"]},{"output_type":"stream","name":"stdout","text":["Feature matrix shape - DINOv2: (4614, 768)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from PIL import Image\n","from tqdm import tqdm\n","import numpy as np\n","\n","def extract_features(image_paths, model, feature_dim, transform, model_name):\n","    features = []\n","    with torch.no_grad():\n","        for path in tqdm(image_paths, desc=f\"Extracting features - {model_name}\"):\n","            try:\n","                img = Image.open(path).convert(\"RGB\")\n","                tensor = transform(img).unsqueeze(0).to(device)\n","                feat = model(tensor).squeeze().cpu().numpy()\n","                features.append(feat)\n","            except Exception as e:\n","                print(f\"Error with {path}: {e}\")\n","                features.append(np.zeros(feature_dim))\n","    return np.array(features)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","model_configs = [\n","    {\n","        'name': 'ResNeXt-101',\n","        'loader': lambda: models.resnext101_32x8d(weights=models.ResNeXt101_32X8D_Weights.DEFAULT),\n","        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n","        'transform': models.ResNet50_Weights.DEFAULT.transforms(),\n","        'feature_dim': 2048\n","    },\n","    {\n","        'name': 'EfficientNet V2',\n","        'loader': lambda: models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.DEFAULT),\n","        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n","        'transform': models.EfficientNet_V2_M_Weights.DEFAULT.transforms(),\n","        'feature_dim': 1280\n","    },\n","    {\n","        'name': 'ConvNeXt',\n","        'loader': lambda: models.convnext_base(weights=models.ConvNeXt_Base_Weights.DEFAULT),\n","        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n","        'transform': models.ConvNeXt_Base_Weights.DEFAULT.transforms(),\n","        'feature_dim': 1024\n","    },\n","    {\n","        'name': 'ViT',\n","        'loader': lambda: models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT),\n","        'modifier': lambda m: (setattr(m.heads, 'head', torch.nn.Identity()), m)[1],\n","        'transform': models.ViT_B_16_Weights.DEFAULT.transforms(),\n","        'feature_dim': 768\n","    },\n","    {\n","        'name': 'Swin Transformer',\n","        'loader': lambda: models.swin_v2_b(weights=models.Swin_V2_B_Weights.DEFAULT),\n","        'modifier': lambda m: torch.nn.Sequential(*list(m.children())[:-1]),\n","        'transform': models.Swin_B_Weights.DEFAULT.transforms(),\n","        'feature_dim': 1024\n","    },\n","    {\n","        'name': 'DINOv2',\n","        'loader': lambda: torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14'),\n","        'modifier': lambda m: m,  # No modification needed\n","        'transform': transforms.Compose([\n","            transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ]),\n","        'feature_dim': 768\n","    }\n","]\n","\n","loaded_models = []\n","for config in model_configs:\n","    model = config['loader']()\n","    model = config['modifier'](model)\n","    model = model.to(device)\n","    model.eval()\n","    loaded_models.append({\n","        'model': model,\n","        'name': config['name'],\n","        'transform': config['transform'],\n","        'feature_dim': config['feature_dim']\n","    })\n","\n","# Extract features for all models on train set\n","all_features = {}\n","for model_info in loaded_models:\n","    features = extract_features(\n","        image_paths_dict['train'],\n","        model_info['model'],\n","        model_info['feature_dim'],\n","        model_info['transform'],\n","        model_info['name']\n","    )\n","    all_features[model_info['name']] = features\n","    print(f\"Feature matrix shape - {model_info['name']}: {features.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"5jJde8topGt6"},"source":["## \"Training\" the KNN Classifiers\n","\n","KNN is trained (fitted) using a simple distance-based rule:\n","- Each image is classified based on the majority vote of its *k* nearest neighbors in the feature space.\n","- We use `k=5` neighbors for this experiment.\n","\n","After training, we compute accuracy."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ksSk59glBFq","outputId":"c8229114-29d3-42e8-91be-07161d2d1d28","executionInfo":{"status":"ok","timestamp":1768726532432,"user_tz":360,"elapsed":4782,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Trained Multi-label KNN for ResNeXt-101\n","Trained Multi-label KNN for EfficientNet V2\n","Trained Multi-label KNN for ConvNeXt\n","Trained Multi-label KNN for ViT\n","Trained Multi-label KNN for Swin Transformer\n","Trained Multi-label KNN for DINOv2\n"]}],"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import accuracy_score, f1_score\n","import numpy as np\n","\n","RESULTS_PATH = os.path.join(HOME, \"runs\", \"classify\")\n","os.makedirs(RESULTS_PATH, exist_ok=True)\n","\n","y_train = labels_dict['train']\n","\n","# Train KNN classifiers with MultiOutputClassifier for all models\n","trained_models = {}\n","for model_name, features in all_features.items():\n","    base_knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n","    multi_label_knn = MultiOutputClassifier(base_knn, n_jobs=-1)\n","    multi_label_knn.fit(features, y_train)\n","\n","    trained_models[model_name] = multi_label_knn\n","    print(f\"Trained Multi-label KNN for {model_name}\")\n","\n","\n","def evaluate_model(X, y, split_name, model, model_name):\n","    split_dir = os.path.join(RESULTS_PATH, split_name)\n","    os.makedirs(split_dir, exist_ok=True)\n","    y_pred = model.predict(X)\n","\n","    # Subset Accuracy (exact match ratio)\n","    subset_accuracy = accuracy_score(y, y_pred)\n","\n","    # Micro F1 (aggregate across all label-sample pairs)\n","    micro_f1 = f1_score(y, y_pred, average='micro', zero_division=0)\n","\n","    # Macro F1 (average F1 across labels)\n","    macro_f1 = f1_score(y, y_pred, average='macro', zero_division=0)\n","\n","    return {\n","        'subset_accuracy': subset_accuracy,\n","        'micro_f1': micro_f1,\n","        'macro_f1': macro_f1,\n","        'y_pred': y_pred\n","    }"]},{"cell_type":"markdown","metadata":{"id":"PTkeOOmipW6i"},"source":["## Predictions on valid set"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Gqv0MJIlDSC","outputId":"f8d9fbfa-4ad2-4eaa-f200-13862e768d03","executionInfo":{"status":"ok","timestamp":1768726614198,"user_tz":360,"elapsed":81763,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Extracting features - ResNeXt-101: 100%|██████████| 440/440 [00:11<00:00, 37.98it/s]\n","Extracting features - EfficientNet V2: 100%|██████████| 440/440 [00:16<00:00, 26.76it/s]\n","Extracting features - ConvNeXt: 100%|██████████| 440/440 [00:08<00:00, 50.93it/s]\n","Extracting features - ViT: 100%|██████████| 440/440 [00:08<00:00, 53.64it/s]\n","Extracting features - Swin Transformer: 100%|██████████| 440/440 [00:18<00:00, 23.61it/s]\n","Extracting features - DINOv2: 100%|██████████| 440/440 [00:11<00:00, 38.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ResNeXt-101 - Valid Set:\n","  Subset Accuracy: 0.7409\n","  Micro F1:        0.8153\n","  Macro F1:        0.7816\n","\n","EfficientNet V2 - Valid Set:\n","  Subset Accuracy: 0.7455\n","  Micro F1:        0.8142\n","  Macro F1:        0.7957\n","\n","ConvNeXt - Valid Set:\n","  Subset Accuracy: 0.7455\n","  Micro F1:        0.8089\n","  Macro F1:        0.7850\n","\n","ViT - Valid Set:\n","  Subset Accuracy: 0.7591\n","  Micro F1:        0.8289\n","  Macro F1:        0.8083\n","\n","Swin Transformer - Valid Set:\n","  Subset Accuracy: 0.7500\n","  Micro F1:        0.8310\n","  Macro F1:        0.8187\n","\n","DINOv2 - Valid Set:\n","  Subset Accuracy: 0.7886\n","  Micro F1:        0.8530\n","  Macro F1:        0.8581\n","\n"]}],"source":["# Extract features for validation set\n","all_valid_features = {}\n","for model_info in loaded_models:\n","    features = extract_features(\n","        image_paths_dict['valid'],\n","        model_info['model'],\n","        model_info['feature_dim'],\n","        model_info['transform'],\n","        model_info['name']\n","    )\n","    all_valid_features[model_info['name']] = features\n","\n","y_valid = labels_dict['valid']\n","\n","# Evaluate all models on valid set\n","for model_name, model in trained_models.items():\n","    results = evaluate_model(all_valid_features[model_name], y_valid, \"valid\", model, model_name)\n","    print(f\"{model_name} - Valid Set:\")\n","    print(f\"  Subset Accuracy: {results['subset_accuracy']:.4f}\")\n","    print(f\"  Micro F1:        {results['micro_f1']:.4f}\")\n","    print(f\"  Macro F1:        {results['macro_f1']:.4f}\")\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"WRwWXMjO8yBe"},"source":["## Metrics on test set"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_juqzFfV7pVH","outputId":"a7a34fc7-81b0-465e-c11e-3f6e6899ea79","executionInfo":{"status":"ok","timestamp":1768726658435,"user_tz":360,"elapsed":44233,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Extracting features - ResNeXt-101: 100%|██████████| 218/218 [00:05<00:00, 40.78it/s]\n","Extracting features - EfficientNet V2: 100%|██████████| 218/218 [00:08<00:00, 25.38it/s]\n","Extracting features - ConvNeXt: 100%|██████████| 218/218 [00:04<00:00, 53.64it/s]\n","Extracting features - ViT: 100%|██████████| 218/218 [00:04<00:00, 53.28it/s]\n","Extracting features - Swin Transformer: 100%|██████████| 218/218 [00:09<00:00, 23.32it/s]\n","Extracting features - DINOv2: 100%|██████████| 218/218 [00:06<00:00, 36.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","TEST SET METRICS - ResNeXt-101\n","==================================================\n","Subset Accuracy: 0.7202\n","Micro F1:        0.7928\n","Macro F1:        0.8051\n","\n","------------------------------\n","F1 SCORE PER CLASS\n","------------------------------\n","french_fries: 0.7879\n","fried_chicken: 0.8421\n","hamburger: 0.8750\n","ice_cream: 0.8500\n","junk_food_logo: 0.7797\n","pizza: 0.8667\n","soda: 0.6341\n","\n","==================================================\n","TEST SET METRICS - EfficientNet V2\n","==================================================\n","Subset Accuracy: 0.7156\n","Micro F1:        0.7959\n","Macro F1:        0.8047\n","\n","------------------------------\n","F1 SCORE PER CLASS\n","------------------------------\n","french_fries: 0.7778\n","fried_chicken: 0.9048\n","hamburger: 0.9091\n","ice_cream: 0.7368\n","junk_food_logo: 0.7904\n","pizza: 0.9091\n","soda: 0.6047\n","\n","==================================================\n","TEST SET METRICS - ConvNeXt\n","==================================================\n","Subset Accuracy: 0.7385\n","Micro F1:        0.8117\n","Macro F1:        0.8319\n","\n","------------------------------\n","F1 SCORE PER CLASS\n","------------------------------\n","french_fries: 0.7692\n","fried_chicken: 0.9268\n","hamburger: 0.9375\n","ice_cream: 0.8000\n","junk_food_logo: 0.7730\n","pizza: 0.8387\n","soda: 0.7778\n","\n","==================================================\n","TEST SET METRICS - ViT\n","==================================================\n","Subset Accuracy: 0.7706\n","Micro F1:        0.8293\n","Macro F1:        0.8343\n","\n","------------------------------\n","F1 SCORE PER CLASS\n","------------------------------\n","french_fries: 0.8485\n","fried_chicken: 0.8780\n","hamburger: 0.9375\n","ice_cream: 0.7805\n","junk_food_logo: 0.8228\n","pizza: 0.8667\n","soda: 0.7059\n","\n","==================================================\n","TEST SET METRICS - Swin Transformer\n","==================================================\n","Subset Accuracy: 0.7294\n","Micro F1:        0.8128\n","Macro F1:        0.8178\n","\n","------------------------------\n","F1 SCORE PER CLASS\n","------------------------------\n","french_fries: 0.7879\n","fried_chicken: 0.8780\n","hamburger: 0.9375\n","ice_cream: 0.8108\n","junk_food_logo: 0.8049\n","pizza: 0.8387\n","soda: 0.6667\n","\n","==================================================\n","TEST SET METRICS - DINOv2\n","==================================================\n","Subset Accuracy: 0.7936\n","Micro F1:        0.8722\n","Macro F1:        0.8915\n","\n","------------------------------\n","F1 SCORE PER CLASS\n","------------------------------\n","french_fries: 0.8947\n","fried_chicken: 0.9524\n","hamburger: 0.9091\n","ice_cream: 0.8780\n","junk_food_logo: 0.8409\n","pizza: 0.9655\n","soda: 0.8000\n"]}],"source":["all_test_features = {}\n","for model_info in loaded_models:\n","    features = extract_features(\n","        image_paths_dict['test'],\n","        model_info['model'],\n","        model_info['feature_dim'],\n","        model_info['transform'],\n","        model_info['name']\n","    )\n","    all_test_features[model_info['name']] = features\n","\n","y_test = labels_dict['test']\n","\n","# Evaluate all models on test set\n","for model_name, model in trained_models.items():\n","    results = evaluate_model(all_test_features[model_name], y_test, \"test\", model, model_name)\n","    print(\"\\n\" + \"=\" * 50)\n","    print(f\"TEST SET METRICS - {model_name}\")\n","    print(\"=\" * 50)\n","    print(f\"Subset Accuracy: {results['subset_accuracy']:.4f}\")\n","    print(f\"Micro F1:        {results['micro_f1']:.4f}\")\n","    print(f\"Macro F1:        {results['macro_f1']:.4f}\")\n","\n","    # F1 score per class\n","    print(\"\\n\" + \"-\" * 30)\n","    print(\"F1 SCORE PER CLASS\")\n","    print(\"-\" * 30)\n","    f1_per_class = f1_score(y_test, results['y_pred'], average=None, zero_division=0)\n","    for class_name, f1 in zip(classes, f1_per_class):\n","        print(f\"{class_name}: {f1:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"0jrI0Z0ZJ6qo"},"source":["## Real images test\n","\n","Let's test the best model (DINOv2) on random images from the test set with multi-label prediction."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1x4JymFxbfvkLbIgji5enTG102vOBQaHL"},"id":"u-eT4HWoJ6qo","executionInfo":{"status":"ok","timestamp":1768726658780,"user_tz":360,"elapsed":343,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"1bfa1548-8769-4553-b6fd-7b2d7dd09c35"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import random\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","from PIL import Image as PILImage\n","\n","# Use the best performing model (DINOv2)\n","best_model_name = \"DINOv2\"\n","best_model = trained_models[best_model_name]\n","\n","# Get the model info for DINOv2\n","dinov2_info = None\n","for model_info in loaded_models:\n","    if model_info['name'] == best_model_name:\n","        dinov2_info = model_info\n","        break\n","\n","# Get indices of images that have at least one label (interesting images)\n","positive_indices = [\n","    i for i, labels_vector in enumerate(labels_dict['test'])\n","    if sum(labels_vector) > 0  # Has at least one label\n","]\n","\n","# Pick 5 random test images with labels\n","test_indices = random.sample(positive_indices, min(5, len(positive_indices)))\n","\n","# Define colors for each class\n","class_colors = {\n","    \"french_fries\": \"#F39C12\",\n","    \"fried_chicken\": \"#E67E22\",\n","    \"hamburger\": \"#8B4513\",\n","    \"ice_cream\": \"#96CEB4\",\n","    \"junk_food_logo\": \"#FFEAA7\",\n","    \"pizza\": \"#FD79A8\",\n","    \"soda\": \"#A29BFE\"\n","}\n","\n","for idx, random_idx in enumerate(test_indices, 1):\n","    random_image_path = image_paths_dict['test'][random_idx]\n","    true_labels_vector = labels_dict['test'][random_idx]\n","    true_labels = [classes[i] for i, val in enumerate(true_labels_vector) if val == 1]\n","\n","    print(f\"{'='*60}\")\n","    print(f\"Image {idx}/5: {random_image_path}\")\n","    print('='*60)\n","\n","    # Load and preprocess the image\n","    image = PILImage.open(random_image_path).convert(\"RGB\")\n","    image_tensor = dinov2_info['transform'](image).unsqueeze(0).to(device)\n","\n","    # Extract features\n","    with torch.no_grad():\n","        features = dinov2_info['model'](image_tensor)\n","        features = features.cpu().numpy().flatten().reshape(1, -1)\n","\n","    # Predict\n","    y_pred = best_model.predict(features)[0]\n","    predicted_labels = [classes[i] for i, val in enumerate(y_pred) if val == 1]\n","\n","    # Display the image with predictions\n","    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n","    ax.imshow(image)\n","    ax.axis(\"off\")\n","\n","    # Create label badges at the bottom\n","    num_labels = len(predicted_labels) if predicted_labels else 1\n","    badge_width = 0.18\n","    badge_spacing = 0.02\n","    total_width = num_labels * badge_width + (num_labels - 1) * badge_spacing\n","    start_x = 0.5 - total_width / 2\n","\n","    if predicted_labels:\n","        for i, label in enumerate(predicted_labels):\n","            x_pos = start_x + i * (badge_width + badge_spacing)\n","            color = class_colors.get(label, \"#95A5A6\")\n","\n","            badge = mpatches.FancyBboxPatch(\n","                (x_pos, -0.08), badge_width, 0.05,\n","                boxstyle=\"round,pad=0.01\",\n","                facecolor=color,\n","                edgecolor=\"white\",\n","                linewidth=2,\n","                transform=ax.transAxes,\n","                clip_on=False\n","            )\n","            ax.add_patch(badge)\n","            ax.text(\n","                x_pos + badge_width / 2, -0.055,\n","                label.upper(),\n","                transform=ax.transAxes,\n","                fontsize=9,\n","                fontweight=\"bold\",\n","                color=\"white\" if label != \"junk_food_logo\" else \"black\",\n","                ha=\"center\",\n","                va=\"center\"\n","            )\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(f\"Model: {best_model_name}\")\n","    print(f\"Predicted labels: {predicted_labels if predicted_labels else '(none)'}\")\n","    print(f\"True labels: {true_labels if true_labels else '(none)'}\")\n","\n","    correct_preds = set(predicted_labels) & set(true_labels)\n","    false_positives = set(predicted_labels) - set(true_labels)\n","    false_negatives = set(true_labels) - set(predicted_labels)\n","    print(f\"Correct predictions: {list(correct_preds) if correct_preds else '(none)'}\")\n","    print(f\"False positives: {list(false_positives) if false_positives else '(none)'}\")\n","    print(f\"False negatives: {list(false_negatives) if false_negatives else '(none)'}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}