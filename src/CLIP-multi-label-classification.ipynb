{"cells":[{"cell_type":"markdown","metadata":{"id":"EnzsjpMSAzB3"},"source":["# Junk Food Detection with CLIP using ViT\n","\n","This pipeline implements a junk food image classification system using the CLIP model. It downloads a COCO-format dataset from Roboflow, merges annotations, preprocesses data, runs inference using CLIP, and evaluates performance with metrics such as accuracy, precision, recall, and confusion matrices."]},{"cell_type":"markdown","metadata":{"id":"6h0eXidyzCEK"},"source":["## Before you start\n","\n","Make sure you have access to GPU. In case of any problems, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, click `Save` and try again."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161,"status":"ok","timestamp":1766911517480,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"},"user_tz":360},"id":"pZdVia1hzNQK","outputId":"763c7a5b-8a7f-47ed-ec0a-3039cb193b5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Dec 28 08:45:17 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   65C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67,"status":"ok","timestamp":1766911517549,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"},"user_tz":360},"id":"iiA-yIY_Sntv","outputId":"9989dec8-97df-4afa-816a-f368542ed052"},"outputs":[{"output_type":"stream","name":"stdout","text":["HOME: /content\n","/content/datasets\n"]}],"source":["import os\n","HOME = os.getcwd()\n","print(\"HOME:\", HOME)\n","!mkdir -p {HOME}/datasets\n","%cd {HOME}/datasets"]},{"cell_type":"markdown","metadata":{"id":"cMwNCMGkzXbo"},"source":["## Install packages using pip\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12739,"status":"ok","timestamp":1766911530293,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"},"user_tz":360},"id":"P8Sq8dbzyRSl","outputId":"50674d71-7c34-4225-b697-9e3edaf32a5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting roboflow==1.2.11\n","  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n","Collecting open-clip-torch==3.2.0\n","  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n","Requirement already satisfied: pillow==11.3.0 in /usr/local/lib/python3.12/dist-packages (11.3.0)\n","Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: torchvision==0.24.0 in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2025.11.12)\n","Collecting idna==3.7 (from roboflow==1.2.11)\n","  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.4.9)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (3.10.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.0.2)\n","Collecting opencv-python-headless==4.10.0.84 (from roboflow==1.2.11)\n","  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Collecting pi-heif<2 (from roboflow==1.2.11)\n","  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n","Collecting pillow-avif-plugin<2 (from roboflow==1.2.11)\n","  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.9.0.post0)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.2.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.32.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.17.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (2.5.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (4.67.1)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (6.0.3)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow==1.2.11) (1.0.0)\n","Collecting filetype (from roboflow==1.2.11)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==3.2.0) (2025.11.3)\n","Collecting ftfy (from open-clip-torch==3.2.0)\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==3.2.0) (0.36.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==3.2.0) (0.7.0)\n","Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open-clip-torch==3.2.0) (1.0.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0) (1.3.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open-clip-torch==3.2.0) (0.2.14)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch==3.2.0) (25.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch==3.2.0) (1.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0) (3.0.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (1.3.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (4.61.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow==1.2.11) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow==1.2.11) (3.4.4)\n","Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, ftfy, roboflow, open-clip-torch\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.12.0.88\n","    Uninstalling opencv-python-headless-4.12.0.88:\n","      Successfully uninstalled opencv-python-headless-4.12.0.88\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.11\n","    Uninstalling idna-3.11:\n","      Successfully uninstalled idna-3.11\n","Successfully installed filetype-1.2.0 ftfy-6.3.1 idna-3.7 open-clip-torch-3.2.0 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"]}],"source":["!pip install roboflow==1.2.11 open-clip-torch==3.2.0 pillow==11.3.0 torch==2.9.0 torchvision==0.24.0"]},{"cell_type":"markdown","metadata":{"id":"dQ1JN1BwQJSy"},"source":["## Download dataset from Roboflow\n","\n","Don't forget to change the `API_KEY` with your dataset key.\n","\n","The dataset from Roboflow comes in COCO format"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24631,"status":"ok","timestamp":1766911554930,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"},"user_tz":360},"id":"SwEc0qz7QJlC","outputId":"056fc127-2774-4258-fa3f-62e3cfe7f9f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading Dataset Version Zip in Junk-Food-Detection-10 to coco:: 100%|██████████| 293482/293482 [00:17<00:00, 17259.73it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n","Extracting Dataset Version Zip to Junk-Food-Detection-10 in coco:: 100%|██████████| 5280/5280 [00:01<00:00, 4384.40it/s]\n"]}],"source":["from roboflow import Roboflow\n","from google.colab import userdata\n","\n","rf = Roboflow(api_key=userdata.get('ROBOFLOW_API_KEY'))\n","project = rf.workspace(userdata.get('ROBOFLOW_WORKSPACE_ID')).project(userdata.get('ROBOFLOW_PROJECT_ID'))\n","version = project.version(userdata.get('ROBOFLOW_DATASET_VERSION'))\n","dataset = version.download(\"coco\")\n","\n","DATASET_PATH = \"datasets/Junk-Food-Detection-10/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1766911555182,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"},"user_tz":360},"id":"58gaxSw32zPx","outputId":"5c589767-0ece-43f5-9eae-9c90dc7c1858"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["%cd {HOME}"]},{"cell_type":"markdown","metadata":{"id":"ZJ_kzpGoD3Ht"},"source":["## Classes recollection\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awjCo_sbD2o3","executionInfo":{"status":"ok","timestamp":1766911555279,"user_tz":360,"elapsed":95,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4d6af0ee-228b-45aa-b786-d8d592cf9daf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Classes: ['junk-food', 'french_fries', 'fried_chicken', 'hamburger', 'ice_cream', 'junk_food_logo', 'pizza', 'soda']\n"]}],"source":["import json\n","from pathlib import Path\n","\n","folders = ['train', 'test', 'valid']\n","all_categories = {}\n","\n","for folder in folders:\n","    json_path = DATASET_PATH + folder + \"/_annotations.coco.json\"\n","\n","    try:\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","            categories = data.get('categories', [])\n","\n","            for category in categories:\n","                cat_id = category['id']\n","                cat_name = category['name']\n","                cat_supercategory = category.get('supercategory', 'none')\n","\n","                # Skip the junk-food category for this particular dataset (it means nothing)\n","                if cat_name == 'junk-food':\n","                  continue\n","\n","                if cat_name not in all_categories:\n","                    all_categories[cat_name] = {\n","                        'id': cat_id,\n","                        'name': cat_name,\n","                        'supercategory': cat_supercategory\n","                    }\n","\n","    except FileNotFoundError:\n","        print(f\"Warning: {json_path} not found\")\n","    except json.JSONDecodeError:\n","        print(f\"Error: {json_path} is not a valid JSON file\")\n","\n","NATURAL_LANGUAGE_TO_CLASS_MAP = {\n","  'junk food': 'junk-food',\n","  'french fries': 'french_fries',\n","  'fried chicken': 'fried_chicken',\n","  'hamburger': 'hamburger',\n","  'ice cream': 'ice_cream',\n","  'junk food logo': 'junk_food_logo',\n","  'pizza': 'pizza',\n","  'soda': 'soda'\n","}\n","CLASS_TO_NATURAL_LANGUAGE_MAP = {v: k for k, v in NATURAL_LANGUAGE_TO_CLASS_MAP.items()}\n","\n","classes_from_dataset = [cat_name for cat_name in CLASS_TO_NATURAL_LANGUAGE_MAP.keys()]\n","\n","print(\"Classes:\", classes_from_dataset)"]},{"cell_type":"markdown","metadata":{"id":"m9a8DSyb6GVQ"},"source":["## Prepare OpenCLIP model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["8596e411c2df47ed891ca59db072b8ad","895e9c155e76496aa645e7a37276a16e","f7989de6433b4585b22d4489a97d7d37","085a097752984dbb819bfc70e85a64e5","85e8bb2364224fc7be54ee0ff43c2124","ba98ae31bba1445dbe0c3a4e59ff0b41","3852e5c1d4b648258098677455dd120b","d4398483d65b4f128d7235de41a1e757","a674ad0778574085be7f4f264db8bcd9","0a613fe08cb64522bf0e27ed37a9605c","29f1919fe4144b00b73044398bdda5c8"]},"executionInfo":{"elapsed":29244,"status":"ok","timestamp":1766911584525,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"},"user_tz":360},"id":"TFOq4krq6aJk","outputId":"2d9dcb31-ab20-4af1-a86c-e8debf38e0fd"},"outputs":[{"output_type":"stream","name":"stderr","text":["\n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n"]},{"output_type":"display_data","data":{"text/plain":["open_clip_model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8596e411c2df47ed891ca59db072b8ad"}},"metadata":{}}],"source":["import torch\n","import open_clip\n","from PIL import Image\n","from collections import defaultdict\n","import json\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, _, preprocess = open_clip.create_model_and_transforms(\n","    \"ViT-B-16\",\n","    pretrained=\"laion2b_s34b_b88k\"\n",")\n","tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n","model = model.to(device)\n","model.eval()\n","\n","def classify_images_with_clip(dataset_part, classes):\n","\n","    with open(DATASET_PATH + dataset_part + \"/\" + \"_annotations.coco.json\", \"r\") as f:\n","        coco_data = json.load(f)\n","\n","    # Build mappings\n","    category_id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in coco_data[\"categories\"]}\n","    image_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in coco_data[\"images\"]}\n","\n","    # Group annotations by image_id\n","    image_annotations = defaultdict(list)\n","    for ann in coco_data[\"annotations\"]:\n","        image_id = ann[\"image_id\"]\n","        category_name = category_id_to_name[ann[\"category_id\"]]\n","        image_annotations[image_id].append(category_name)\n","\n","    # Get ground truth labels per image (unique categories)\n","    ground_truth = {}\n","    for image_id, categories in image_annotations.items():\n","        filename = image_id_to_filename[image_id]\n","        unique_categories = list(set(categories))\n","        ground_truth[filename] = unique_categories\n","\n","    # Prompt ensembling\n","    templates = [\n","        \"a photo of {}\",\n","        \"a picture of {}\",\n","        \"an image containing {}\",\n","        \"a close-up photo of {}\",\n","        \"an ad containing {}\"\n","    ]\n","\n","    # Encode text features once\n","    with torch.no_grad():\n","        text_features = []\n","\n","        for cls in classes:\n","            prompts = [t.format(CLASS_TO_NATURAL_LANGUAGE_MAP[cls]) for t in templates]\n","            tokens = tokenizer(prompts).to(device)\n","\n","            embeddings = model.encode_text(tokens)\n","            embeddings /= embeddings.norm(dim=-1, keepdim=True)\n","\n","            class_embedding = embeddings.mean(dim=0)\n","            class_embedding /= class_embedding.norm()\n","\n","            text_features.append(class_embedding)\n","\n","        text_features = torch.stack(text_features)  # [num_classes, embed_dim]\n","\n","    # Multi-label threshold (cosine similarity space)\n","    classification_threshold = 0.2  # Adjust this based on your data\n","\n","    results = {}\n","    image_paths = [img[\"file_name\"] for img in coco_data[\"images\"]]\n","\n","    for image_path in image_paths:\n","        try:\n","            image = Image.open(DATASET_PATH + dataset_part + \"/\" + image_path).convert(\"RGB\")\n","            image_tensor = preprocess(image).unsqueeze(0).to(device)\n","\n","            with torch.no_grad():\n","                image_features = model.encode_image(image_tensor)\n","                image_features /= image_features.norm(dim=-1, keepdim=True)\n","\n","                # Cosine similarity per class\n","                similarities = (image_features @ text_features.T).squeeze(0).cpu()\n","\n","            # Independent multi-label decision\n","            predicted_labels = []\n","            predicted_scores = {}\n","\n","            for idx, score in enumerate(similarities):\n","                score_value = score.item()\n","                predicted_scores[classes[idx]] = score_value\n","\n","                if score_value >= classification_threshold:\n","                    predicted_labels.append(classes[idx])\n","\n","            # Store all predicted labels and scores\n","            results[image_path] = {\n","                \"labels\": predicted_labels,\n","                \"scores\": predicted_scores\n","            }\n","\n","        except FileNotFoundError:\n","            raise FileNotFoundError(\n","                f\"Image file not found: {image_path}. Cannot continue processing.\"\n","            )\n","\n","    return results, ground_truth, image_paths"]},{"cell_type":"markdown","metadata":{"id":"GpHdZm1xmQCc"},"source":["## Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_9F9TFwmPb0"},"outputs":[],"source":["def evaluate_predictions(image_paths, ground_truth, results, classes):\n","    # Per-class metrics\n","    class_metrics = {cls: {'tp': 0, 'fp': 0, 'fn': 0} for cls in classes}\n","\n","    # Overall metrics (micro-averaged)\n","    total_tp = 0\n","    total_fp = 0\n","    total_fn = 0\n","\n","    for image_path in image_paths:\n","        true_labels = set(ground_truth.get(image_path, []))\n","        pred_entry = results.get(image_path, {})\n","        pred_labels = set(pred_entry.get(\"labels\", []))\n","\n","        # Evaluate each class independently\n","        for cls in classes:\n","            true_positive = cls in true_labels\n","            pred_positive = cls in pred_labels\n","\n","            if true_positive and pred_positive:\n","                class_metrics[cls]['tp'] += 1\n","                total_tp += 1\n","            elif not true_positive and pred_positive:\n","                class_metrics[cls]['fp'] += 1\n","                total_fp += 1\n","            elif true_positive and not pred_positive:\n","                class_metrics[cls]['fn'] += 1\n","                total_fn += 1\n","\n","    # Calculate macro-F1\n","    macro_f1 = 0\n","    for cls in classes:\n","        tp = class_metrics[cls]['tp']\n","        fp = class_metrics[cls]['fp']\n","        fn = class_metrics[cls]['fn']\n","\n","        precision = (tp / (tp + fp)) if (tp + fp) > 0 else 0\n","        recall = (tp / (tp + fn)) if (tp + fn) > 0 else 0\n","        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n","        macro_f1 += f1\n","\n","    macro_f1 = (macro_f1 / len(classes) * 100) if len(classes) > 0 else 0\n","\n","    # Calculate micro-F1\n","    micro_precision = (total_tp / (total_tp + total_fp)) if (total_tp + total_fp) > 0 else 0\n","    micro_recall = (total_tp / (total_tp + total_fn)) if (total_tp + total_fn) > 0 else 0\n","    micro_f1 = (2 * micro_precision * micro_recall / (micro_precision + micro_recall)) if (micro_precision + micro_recall) > 0 else 0\n","\n","    # Calculate subset accuracy (exact match)\n","    exact_matches = sum(\n","        1 for path in image_paths\n","        if set(ground_truth.get(path, [])) == set(results.get(path, {}).get(\"labels\", []))\n","    )\n","    subset_accuracy = (exact_matches / len(image_paths) * 100) if len(image_paths) > 0 else 0\n","\n","    return {\n","        'micro_f1_score': micro_f1 * 100,\n","        'macro_f1_score': macro_f1,\n","        'subset_accuracy': subset_accuracy\n","    }"]},{"cell_type":"markdown","source":["## Prediction on validation set"],"metadata":{"id":"MfEu3iDK98II"}},{"cell_type":"code","source":["valid_results, valid_ground_truth, valid_image_paths = classify_images_with_clip(\n","    dataset_part=\"valid\",\n","    classes=classes_from_dataset,\n",")\n","\n","valid_metrics = evaluate_predictions(\n","    image_paths=valid_image_paths,\n","    ground_truth=valid_ground_truth,\n","    results=valid_results,\n","    classes=classes_from_dataset,\n",")\n","\n","print(f\"Subset Accuracy: {valid_metrics['subset_accuracy']:.2f}%\")\n","print(f\"Micro F1: {valid_metrics['micro_f1_score']:.2f}%\")\n","print(f\"Macro F1: {valid_metrics['macro_f1_score']:.2f}%\")"],"metadata":{"id":"DMa8ZtUL-Cuc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766911598692,"user_tz":360,"elapsed":14129,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"e49514e7-2423-45f9-9a68-21ac9345ab9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Subset Accuracy: 43.18%\n","Micro F1: 44.60%\n","Macro F1: 48.56%\n"]}]},{"cell_type":"markdown","source":["## Run model on test set"],"metadata":{"id":"ySrLsbRGZNmZ"}},{"cell_type":"code","source":["test_results, test_ground_truth, test_image_paths = classify_images_with_clip(\n","    dataset_part=\"test\",\n","    classes=classes_from_dataset,\n",")\n","\n","test_metrics = evaluate_predictions(\n","    image_paths=test_image_paths,\n","    ground_truth=test_ground_truth,\n","    results=test_results,\n","    classes=classes_from_dataset,\n",")\n","\n","print(f\"Subset Accuracy: {test_metrics['subset_accuracy']:.2f}%\")\n","print(f\"Micro F1: {test_metrics['micro_f1_score']:.2f}%\")\n","print(f\"Macro F1: {test_metrics['macro_f1_score']:.2f}%\")"],"metadata":{"id":"c7RaDCFPZYXd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766911603138,"user_tz":360,"elapsed":4443,"user":{"displayName":"Jose Andrés","userId":"08637818255854367082"}},"outputId":"196e3b6e-cd69-4d8e-a2ab-7cc9df4c05d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Subset Accuracy: 42.20%\n","Micro F1: 43.24%\n","Macro F1: 48.99%\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8596e411c2df47ed891ca59db072b8ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_895e9c155e76496aa645e7a37276a16e","IPY_MODEL_f7989de6433b4585b22d4489a97d7d37","IPY_MODEL_085a097752984dbb819bfc70e85a64e5"],"layout":"IPY_MODEL_85e8bb2364224fc7be54ee0ff43c2124"}},"895e9c155e76496aa645e7a37276a16e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba98ae31bba1445dbe0c3a4e59ff0b41","placeholder":"​","style":"IPY_MODEL_3852e5c1d4b648258098677455dd120b","value":"open_clip_model.safetensors: 100%"}},"f7989de6433b4585b22d4489a97d7d37":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4398483d65b4f128d7235de41a1e757","max":598516980,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a674ad0778574085be7f4f264db8bcd9","value":598516980}},"085a097752984dbb819bfc70e85a64e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a613fe08cb64522bf0e27ed37a9605c","placeholder":"​","style":"IPY_MODEL_29f1919fe4144b00b73044398bdda5c8","value":" 599M/599M [00:07&lt;00:00, 74.8MB/s]"}},"85e8bb2364224fc7be54ee0ff43c2124":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba98ae31bba1445dbe0c3a4e59ff0b41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3852e5c1d4b648258098677455dd120b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4398483d65b4f128d7235de41a1e757":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a674ad0778574085be7f4f264db8bcd9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a613fe08cb64522bf0e27ed37a9605c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29f1919fe4144b00b73044398bdda5c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}